{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Jupyter Notebook to Learn about faster Optimizers and visualize them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "1. Start by executing the ``import`` Block to see, if the required packages are installed. Install missing ones until you can successfully execute the block\n",
    "2. Go through the Sections. Do the following steps for each section\n",
    "    1. Execute the `Setup` block. This creates all necessary functions for the section\n",
    "    2. Work on the `Task` block\n",
    "    3. Visualize and Test using the `Visualize` Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from typing import Tuple, List, Callable, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Different Functions with different shapes\n",
    "def steep_valley_function():\n",
    "    def f(x, y):\n",
    "        # Define a function with a steeper gradient in the y-direction\n",
    "        return 0.5 * x**2 + 2 * y**2\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Compute the gradient of the function\n",
    "        grad_x = x  # Gradient with respect to x\n",
    "        grad_y = 4 * y  # Gradient with respect to y, steeper than x\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def l_shaped_valley_function():\n",
    "    def f(x, y):\n",
    "        # Creating an L-shaped valley with a sharp turn\n",
    "        # Define the valley with two different linear regions\n",
    "        # Horizontal part (x > y), Vertical part (x <= y)\n",
    "        return np.where(x > y, (y + 20) ** 2 + x, (x + 20) ** 2 + y)\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Gradient of the L-shaped function, changing at x = y\n",
    "        # Horizontal part gradient\n",
    "        grad_x = np.where(x > y, 1, 2 * (x + 20))\n",
    "        grad_y = np.where(x > y, 2 * (y + 20), 1)\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-50, 20, 400)\n",
    "    y = np.linspace(-50, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def asymmetric_convex_function():\n",
    "    def f(x, y):\n",
    "        # Scale the coefficients to increase the depth and curvature differences\n",
    "        return (\n",
    "            0.3 * (x - 10) ** 2\n",
    "            + 0.2 * (y - 10) ** 2\n",
    "            + 1.1 * (x + 10) ** 2\n",
    "            + 1 * (y + 10) ** 2\n",
    "        )\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Adjusted gradient for the scaled function\n",
    "        grad_x = 0.6 * (x - 10) + 2.2 * (\n",
    "            x + 10\n",
    "        )  # Increased coefficients for the gradients\n",
    "        grad_y = 0.4 * (y - 10) + 2 * (y + 10)\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def symmetric_convex_function():\n",
    "    def f(x, y):\n",
    "        return x**2 + y**2  # Z as a function of X and Y\n",
    "\n",
    "    def grad(x, y):\n",
    "        grad_x = 2 * x\n",
    "        grad_y = 2 * y\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 100)\n",
    "    y = np.linspace(-20, 20, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "# defining General optimize function to standardize paramters and return\n",
    "def optimize(\n",
    "    # parent method to call all other optimization methods\n",
    "    optimizer_type=\"Regular\",\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    ") -> Tuple[Callable[[float, float], float], List[np.ndarray], List[float], str]:\n",
    "\n",
    "    # Choose and run the optimizer\n",
    "    # they always return this way: X Y Z positions,\n",
    "    if optimizer_type == \"Regular\":\n",
    "        return regular_gradient_descent(function, steps, lr, cur_pos, print_res)\n",
    "    elif optimizer_type == \"Momentum\":\n",
    "        return momentum_gradient_descent(function, steps, lr, beta1, cur_pos, print_res)\n",
    "    elif optimizer_type == \"Nesterov\":\n",
    "        return nesterov_gradient_descent(function, steps, lr, beta1, cur_pos, print_res)\n",
    "    elif optimizer_type == \"AdaGrad\":\n",
    "        return adagrad_gradient_descent(function, steps, lr, cur_pos, print_res)\n",
    "    elif optimizer_type == \"RMSProp\":\n",
    "        return rmsprop_gradient_descent(function, steps, lr, beta1, cur_pos, print_res)\n",
    "    elif optimizer_type == \"Adam\":\n",
    "        return adam_gradient_descent(\n",
    "            function, steps, lr, beta1, beta2, cur_pos, print_res\n",
    "        )\n",
    "\n",
    "\n",
    "def optimize_multiple(\n",
    "    optimizers=[\"Regular\", \"momentum\"],\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    results = []\n",
    "    for optimizer in optimizers:\n",
    "        results.append(\n",
    "            optimize(optimizer, function, steps, lr, beta1, beta2, cur_pos, print_res)\n",
    "        )\n",
    "    return results\n",
    "\n",
    "# Plotting Function that takes a list of results and Returns a Plot (not yet interactive)\n",
    "def plot_optimization_path(optimizer_results):\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax2 = fig.add_subplot(1, 2, 2)  # 2D plot for top-down heatmap view\n",
    "\n",
    "    # Assuming all optimizers use the same function landscape\n",
    "    f = optimizer_results[0][0]  # Extract function handle from first result\n",
    "    x_range = np.linspace(-20, 20, 400)\n",
    "    y_range = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = f(X, Y)\n",
    "\n",
    "    # Plot the 3D surface\n",
    "    ax1.plot_surface(X, Y, Z, alpha=0.5, cmap='viridis', edgecolor='none')\n",
    "    ax1.set_title('3D View - Optimization Paths')\n",
    "\n",
    "    # Plot the 2D heatmap\n",
    "    ax2.contourf(X, Y, Z, levels=100, cmap='viridis')\n",
    "    ax2.set_title('Top-Down View (2D) - Optimization Paths')\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "\n",
    "    # Define specific colors for each optimizer\n",
    "    color_map = {\n",
    "        \"Regular\": \"red\",\n",
    "        \"Momentum\": \"blue\",\n",
    "        \"Nesterov\": \"green\",\n",
    "        \"AdaGrad\": \"orange\",\n",
    "        \"RMSProp\": \"purple\",\n",
    "        \"Adam\": \"black\"\n",
    "    }\n",
    "\n",
    "    # Plot the paths for each optimizer\n",
    "    for idx, (f, positions, z_values, name) in enumerate(optimizer_results):\n",
    "        positions_array = np.array(positions)\n",
    "        color = color_map.get(name, \"gray\")  # Default to gray if name not found\n",
    "        \n",
    "        # 3D Path\n",
    "        ax1.scatter(\n",
    "            positions_array[:, 0], positions_array[:, 1], z_values,\n",
    "            color=color, s=50, label=name\n",
    "        )\n",
    "        \n",
    "        # 2D Path\n",
    "        ax2.plot(\n",
    "            positions_array[:, 0],\n",
    "            positions_array[:, 1],\n",
    "            marker='o',\n",
    "            color=color,\n",
    "            markersize=5,\n",
    "            linestyle='--',\n",
    "            linewidth=2,\n",
    "            label=name\n",
    "        )\n",
    "\n",
    "    ax1.set_xlim(-20,20)\n",
    "    ax1.set_ylim(-20,20)\n",
    "\n",
    "    ax2.set_xlim(-20,20)\n",
    "    ax2.set_ylim(-20,20)\n",
    "\n",
    "    ax1.legend()  # Show legends to label different optimizer paths\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Method that takes plotting method and adds interactive elements\n",
    "def visualize_optimizer(optimizer_multiselect_options=[\"Regular\", \"Momentum\", \"Nesterov\", \"AdaGrad\", \"RMSProp\", \"Adam\"]):\n",
    "    # Define Optimizer Setting Widgets\n",
    "    optimizer_settings = {\n",
    "        \"Regular\": {\"lr\": widgets.FloatSlider(value=0.1, min=0, max=2, step=0.01, description=\"Learning Rate\")},\n",
    "        \"Momentum\": {\n",
    "            \"lr\": widgets.FloatSlider(value=0.1, min=0, max=2, step=0.01, description=\"Learning Rate\"),\n",
    "            \"beta1\": widgets.FloatSlider(value=0.9, min=0, max=1, step=0.01, description=\"Beta1 Factor\")\n",
    "        },\n",
    "        \"Nesterov\": {\n",
    "            \"lr\": widgets.FloatSlider(value=0.1, min=0, max=2, step=0.01, description=\"Learning Rate\"),\n",
    "            \"beta1\": widgets.FloatSlider(value=0.9, min=0, max=1, step=0.01, description=\"Beta1 Factor\")\n",
    "        },\n",
    "        \"AdaGrad\": {\"lr\": widgets.FloatSlider(value=\"0.1\", min=0, max=2, step=0.01, description=\"Learning Rate\")},\n",
    "        \"RMSProp\": {\n",
    "            \"lr\": widgets.FloatSlider(value=\"0.1\", min=0, max=2, step=0.01, description=\"Learning Rate\"),\n",
    "            \"beta1\": widgets.FloatSlider(value=0.9, min=0, max=1, step=0.01, description=\"Beta1 Factor\")\n",
    "        },\n",
    "        \"Adam\": {\n",
    "            \"lr\": widgets.FloatSlider(value=\"0.1\", min=0, max=2, step=0.01, description=\"Learning Rate\"),\n",
    "            \"beta1\": widgets.FloatSlider(value=0.9, min=0, max=1, step=0.01, description=\"Beta1 Factor\"),\n",
    "            \"beta2\": widgets.FloatSlider(value=0.999, min=0, max=1, step=0.001, description=\"Beta2 Factor\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Filter the optimizer_settings based on optimizer_multiselect_options\n",
    "    optimizer_settings = {key: value for key, value in optimizer_settings.items() if key in optimizer_multiselect_options}\n",
    "\n",
    "    # Define Function Selection and Initial Position Widgets\n",
    "    func_dropdown = widgets.Dropdown(\n",
    "        description=\"Function\",\n",
    "        options=[\n",
    "            (\"Steep Valley Function\", steep_valley_function),\n",
    "            (\"Symmetric Convex Function\", symmetric_convex_function),\n",
    "            (\"Asymmetric Convex Function\", asymmetric_convex_function),\n",
    "            (\"L-shaped Valley Function\", l_shaped_valley_function),\n",
    "        ],\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    x_slider = widgets.IntSlider(value=15, min=-20, max=20, step=1, description=\"Starting X\", layout=widgets.Layout(width=\"100%\"))\n",
    "    y_slider = widgets.IntSlider(value=10, min=-20, max=20, step=1, description=\"Starting Y\", layout=widgets.Layout(width=\"100%\"))\n",
    "    steps_slider = widgets.IntSlider(value=10, min=1, max=30, step=1, description=\"Steps\", layout=widgets.Layout(width=\"100%\"))\n",
    "\n",
    "    # Define Optimizer Toggle and Multi-Select Widgets\n",
    "    optimizer_multiselect = widgets.SelectMultiple(\n",
    "        description=\"Optimizers\",\n",
    "        options=list(optimizer_settings.keys()),\n",
    "        value=[list(optimizer_settings.keys())[0]],\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "\n",
    "    # Optimizer Toggle for Parameter Configuration\n",
    "    optimizer_toggle = widgets.ToggleButtons(\n",
    "        description=\"Configure Optimizer\",\n",
    "        options=list(optimizer_settings.keys()),\n",
    "        value=list(optimizer_settings.keys())[0],\n",
    "        layout=widgets.Layout(width=\"100%\")\n",
    "    )\n",
    "\n",
    "    # Placeholder for Dynamic Optimizer Settings\n",
    "    optimizer_settings_box = widgets.VBox()\n",
    "\n",
    "    # Function to Update Optimizer Settings Display\n",
    "    def update_optimizer_settings(change):\n",
    "        selected_optimizer = change['new']\n",
    "        boxes = [widgets.Label(value=selected_optimizer)]\n",
    "        for setting, widget in optimizer_settings[selected_optimizer].items():\n",
    "            boxes.append(widget)\n",
    "        optimizer_settings_box.children = boxes\n",
    "\n",
    "    optimizer_toggle.observe(update_optimizer_settings, names='value')\n",
    "\n",
    "    # Initial Setup for Optimizer Settings Display\n",
    "    update_optimizer_settings({'new': optimizer_toggle.value})\n",
    "\n",
    "    # Function to Update Plot\n",
    "    def update_plot(optimizers, function, steps, initialX, initialY, **kwargs):\n",
    "        cur_pos = np.array([initialX, initialY])\n",
    "        results = []\n",
    "        for optimizer in optimizers:\n",
    "            params = {key: kwargs.get(f'{optimizer}_{key}') for key in optimizer_settings[optimizer].keys()}\n",
    "            results.append(optimize(optimizer, function, steps, cur_pos=cur_pos, **params))\n",
    "        plot_optimization_path(results)\n",
    "\n",
    "    # Interactive Output\n",
    "    out = widgets.interactive_output(\n",
    "        update_plot,\n",
    "        {\n",
    "            \"optimizers\": optimizer_multiselect,\n",
    "            \"function\": func_dropdown,\n",
    "            \"steps\": steps_slider,\n",
    "            \"initialX\": x_slider,\n",
    "            \"initialY\": y_slider,\n",
    "            **{f'{opt}_{param}': widget for opt, settings in optimizer_settings.items() for param, widget in settings.items()}\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Display UI\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([x_slider, y_slider]),\n",
    "        widgets.HBox([steps_slider, func_dropdown]),\n",
    "        optimizer_multiselect,\n",
    "        optimizer_toggle,\n",
    "        optimizer_settings_box\n",
    "    ])\n",
    "\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Regular vs Momentum Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Hier werden die Functions für den Step und Gradient Descent der zwei ersten Optimizer gesetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Regular Gradient Descent Step and Function\n",
    "\n",
    "\n",
    "def regular_gradient_step(pos, lr, grad):\n",
    "    return pos - lr * grad(*pos)\n",
    "\n",
    "\n",
    "def regular_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    ") -> Tuple[Callable[[float, float], float], List[np.ndarray], List[float], str]:\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos = regular_gradient_step(cur_pos, lr, grad)\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # added to prevent weird plots with points out of bound\n",
    "        positions.append(cur_pos)\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Regular\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Momentum Gradient Descent Step and Function\n",
    "\n",
    "\n",
    "def momentum_step(pos, lr, beta, momentum, grad):\n",
    "    gradient = grad(*pos)\n",
    "    momentum = beta * momentum - lr * gradient\n",
    "    pos = pos + momentum\n",
    "\n",
    "    return momentum, pos\n",
    "\n",
    "\n",
    "def momentum_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    momentum = np.zeros(2)\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        momentum, cur_pos = momentum_step(cur_pos, lr, beta1, momentum, grad)\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # added to prevent weird plots with points out of bound\n",
    "        positions.append(cur_pos)\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Momentum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task / Visualize\n",
    "The Task in Section 1 is to play around with the Regular and Momentum optimizers and get comfortable using the visualizer tool.\n",
    "\n",
    "Try answering these Questions (for yourself):\n",
    "- How do the Optimizers behave for different `Starting X` and `Starting Y` values?\n",
    "- How does the Optimizers behaviour change, when the ``learning rate`` is adjusted?\n",
    "- What part of the Optimizers behaviour is influenced by the ``Beta1 Factor``?\n",
    "- How does the Optimizer behave in different ``Functions``?\n",
    "\n",
    "Tips:\n",
    "- You can choose Multiple Optimizers at once by holding down the Shift Key during selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0727295694498899823c02c7972df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a77ba79d3d14a5f893fb1250416d8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_optimizer(optimizer_multiselect_options=[\"Regular\", \"Momentum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Nesterov Accelerated Gradient (NAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Setup)\n",
    "Diese Section bedarf keines weiteren Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Your task is, to define the step function for the Nesterov Accelerated Gradient Descent (short, NAG).\n",
    "\n",
    "Tips:\n",
    "- Look closely at the mathematical definition\n",
    "- Orient yourself at the Momentum Optimizer's Step Function\n",
    "\n",
    "Nesterov Equation:\n",
    "$$ m \\leftarrow \\beta m - \\eta \\nabla_{\\theta} J(\\theta + \\beta m) $$\n",
    "$$ \\theta \\leftarrow \\theta + m $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov_step(pos, lr, beta, momentum, grad):\n",
    "    # TODO Implement the Nesterov Step function\n",
    "    # Important: the parameter grad is a Function giving you the value of the gradient at position x when calling grad(*x)\n",
    "    return pos, momentum\n",
    "\n",
    "\n",
    "def nesterov_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    momentum = np.zeros(2)  # Initial momentum vector\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos, momentum = nesterov_step(cur_pos, lr, beta1, momentum, grad)\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # added to prevent weird plots with points out of bound\n",
    "        positions.append(cur_pos.copy())  # Append the new position after update\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    # Use the generic plotting function to visualize the path\n",
    "    return (f, positions, z_values, \"Nesterov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80adade5890f48b1bfda5b6f50383647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831386806fa94184b5dcd34edb73a2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_optimizer(optimizer_multiselect_options=['Nesterov'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: AdaGrad, RMSProp und Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO hier weiß ich noch nicht, welche Aufgaben wir geben sollen und wie ich die aufbauen soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adagrad_step(pos, lr, grad, s):\n",
    "    epsilon = 10 ** (-10)  # set to different value if needed\n",
    "    gradient = grad(*pos)\n",
    "    s += gradient**2  # Accumulate squared gradients\n",
    "    adjusted_lr = lr / (np.sqrt(s) + epsilon)\n",
    "    pos = pos - adjusted_lr * gradient\n",
    "    return pos, s\n",
    "\n",
    "\n",
    "def adagrad_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    s = np.zeros(2)  # Initialize sum of squares of gradients\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos, s = adagrad_step(cur_pos, lr, grad, s)\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # added to prevent weird plots with points out of bound\n",
    "        positions.append(cur_pos.copy())  # Ensure a deep copy for immutability in list\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"AdaGrad\")\n",
    "\n",
    "\n",
    "def rmsprop_step(pos, lr, grad, s, beta=0.9, epsilon=1e-8):\n",
    "    gradient = grad(*pos)\n",
    "    s = beta * s + (1 - beta) * gradient * gradient\n",
    "    pos = pos - (lr * gradient / (np.sqrt(s + epsilon)))\n",
    "    return pos, s\n",
    "\n",
    "    # # alternative function with adjusted learning rate\n",
    "    # gradient = grad(*pos)\n",
    "    # s = beta * s + (1 - beta) * gradient ** 2  # Update accumulation of squared gradients\n",
    "    # adjusted_lr = lr / (np.sqrt(s) + epsilon)  # Adjust learning rate\n",
    "    # pos = pos - adjusted_lr * gradient  # Update position\n",
    "    # return pos, s\n",
    "\n",
    "\n",
    "def rmsprop_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    s = np.zeros_like(cur_pos)  # Initialize the RMS accumulation variable\n",
    "    for i in range(steps):\n",
    "        cur_pos, s = rmsprop_step(\n",
    "            cur_pos, lr, grad, s, beta1\n",
    "        )  # Update position and RMS\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # added to prevent weird plots with points out of bound\n",
    "        positions.append(cur_pos.copy())  # Ensure a deep copy for immutability in list\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"RMSProp\")\n",
    "\n",
    "\n",
    "def adam_step(pos, lr, grad, m, v, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    gradient = grad(*pos)\n",
    "    m = beta1 * m + (1 - beta1) * gradient  # Update biased first moment estimate\n",
    "    v = beta2 * v + (1 - beta2) * gradient**2  # Update biased second moment estimate\n",
    "    m_hat = m / (1 - beta1**t)  # Compute bias-corrected first moment estimate\n",
    "    v_hat = v / (1 - beta2**t)  # Compute bias-corrected second moment estimate\n",
    "    pos = pos - lr * m_hat / (np.sqrt(v_hat) + epsilon)  # Update parameters\n",
    "    return pos, m, v\n",
    "\n",
    "\n",
    "def adam_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.99,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    m = np.zeros_like(cur_pos)  # Initialize first moment vector\n",
    "    v = np.zeros_like(cur_pos)  # Initialize second moment vector\n",
    "    t = 0  # Initialize timestep\n",
    "\n",
    "    positions = [cur_pos.copy()]  # Store positions for plotting\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        t += 1  # Increment time step\n",
    "        cur_pos, m, v = adam_step(\n",
    "            cur_pos, lr, grad, m, v, t, beta1, beta2\n",
    "        )  # Adam optimization step\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # added to prevent weird plots with points out of bound\n",
    "        positions.append(cur_pos.copy())  # Ensure a deep copy for immutability in list\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    # Use the plot function to visualize the optimization path\n",
    "    return (f, positions, z_values, \"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5733e51da11646429da499fa584a0058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc99fbca5a54008bb7626ad976aff09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section X: Vergleich aller Optimierer\n",
    "In dieser Section sollen noch einmal alle Optimierer interaktiv miteinander verglichen werden.\n",
    "\n",
    "Die Aufgabe ist es XX mit dem Gesamtbild herumzuspielen?(keine gute idee, irgendwie wettkampf einbauen. Vielleicht, indem wir ne Funktion bauen, die die Nähe zum Ziel ausgibt. Dann kann man gucken, wer in einer bestimmten Anzahl Schritte möglichst nahe gekommen ist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283999fed69f4b9183233ce3cf4658e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd7f2caa87242ad90e6f2d176d5c7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_optimizer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_lars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
