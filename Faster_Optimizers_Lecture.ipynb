{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Jupyter Notebook to Learn about faster Optimizers and visualize them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "1. Start by executing the ``import`` Block to see, if the required packages are installed. Install missing ones until you can successfully execute the block\n",
    "2. Go through the Sections. Do the following steps for each section\n",
    "    1. Execute the `Setup` block. This creates all necessary functions for the section\n",
    "    2. Work on the `Task` block\n",
    "    3. Visualize and Test using the `Visualize` Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from typing import Tuple, List, Callable, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Different Functions with different shapes\n",
    "def steep_valley_function():\n",
    "    def f(x, y):\n",
    "        # Define a function with a steeper gradient in the y-direction\n",
    "        return 0.5 * x**2 + 2 * y**2\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Compute the gradient of the function\n",
    "        grad_x = x  # Gradient with respect to x\n",
    "        grad_y = 4 * y  # Gradient with respect to y, steeper than x\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def l_shaped_valley_function():\n",
    "    def f(x, y):\n",
    "        # Creating an L-shaped valley with a sharp turn\n",
    "        # Define the valley with two different linear regions\n",
    "        # Horizontal part (x > y), Vertical part (x <= y)\n",
    "        return np.where(x > y, (y + 20) ** 2 + x, (x + 20) ** 2 + y)\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Gradient of the L-shaped function, changing at x = y\n",
    "        # Horizontal part gradient\n",
    "        grad_x = np.where(x > y, 1, 2 * (x + 20))\n",
    "        grad_y = np.where(x > y, 2 * (y + 20), 1)\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-50, 20, 400)\n",
    "    y = np.linspace(-50, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def complex_function():\n",
    "    def f(x, y):\n",
    "        # Scale the coefficients to increase the depth and curvature differences\n",
    "        return (\n",
    "            0.3 * (x - 10) ** 2\n",
    "            + 0.2 * (y - 10) ** 2\n",
    "            + 1 * (x + 10) ** 2\n",
    "            + 1 * (y + 10) ** 2\n",
    "        )\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Adjusted gradient for the scaled function\n",
    "        grad_x = 0.6 * (x - 10) + 2 * (\n",
    "            x + 10\n",
    "        )  # Increased coefficients for the gradients\n",
    "        grad_y = 0.4 * (y - 10) + 2 * (y + 10)\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def simple_function():\n",
    "    def f(x, y):\n",
    "        return x**2 + y**2  # Z as a function of X and Y\n",
    "\n",
    "    def grad(x, y):\n",
    "        grad_x = 2 * x\n",
    "        grad_y = 2 * y\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 100)\n",
    "    y = np.linspace(-20, 20, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "# defining General optimize function to standardize paramters and return\n",
    "def optimize(\n",
    "    # parent method to call all other optimization methods\n",
    "    optimizer_type=\"Regular\",\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    ") -> Tuple[Callable[[float, float], float], List[np.ndarray], List[float], str]:\n",
    "\n",
    "    # Choose and run the optimizer\n",
    "    # they always return this way: X Y Z positions,\n",
    "    if optimizer_type == \"Regular\":\n",
    "        return regular_gradient_descent(function, steps, lr, cur_pos, print_res)\n",
    "    elif optimizer_type == \"Momentum\":\n",
    "        return momentum_gradient_descent(function, steps, lr, beta1, cur_pos, print_res)\n",
    "    elif optimizer_type == \"Nesterov\":\n",
    "        return nesterov_gradient_descent(function, steps, lr, beta1, cur_pos, print_res)\n",
    "    elif optimizer_type == \"AdaGrad\":\n",
    "        return adagrad_gradient_descent(function, steps, lr, cur_pos, print_res)\n",
    "    elif optimizer_type == \"RMSProp\":\n",
    "        return rmsprop_gradient_descent(function, steps, lr, beta1, cur_pos, print_res)\n",
    "    elif optimizer_type == \"Adam\":\n",
    "        return adam_gradient_descent(\n",
    "            function, steps, lr, beta1, beta2, cur_pos, print_res\n",
    "        )\n",
    "\n",
    "\n",
    "def optimize_multiple(\n",
    "    optimizers=[\"Regular\", \"momentum\"],\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    results = []\n",
    "    for optimizer in optimizers:\n",
    "        results.append(\n",
    "            optimize(optimizer, function, steps, lr, beta1, beta2, cur_pos, print_res)\n",
    "        )\n",
    "    return results\n",
    "\n",
    "# Plotting Function that takes a list of results and Returns a Plot (not yet interactive)\n",
    "def plot_optimization_path(optimizer_results):\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax2 = fig.add_subplot(1, 2, 2)  # 2D plot for top-down heatmap view\n",
    "\n",
    "    # Assuming all optimizers use the same function landscape\n",
    "    f = optimizer_results[0][0]  # Extract function handle from first result\n",
    "    x_range = np.linspace(-20, 20, 400)\n",
    "    y_range = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = f(X, Y)\n",
    "\n",
    "    # Plot the 3D surface\n",
    "    ax1.plot_surface(X, Y, Z, alpha=0.5, cmap='viridis', edgecolor='none')\n",
    "    ax1.set_title('3D View - Optimization Paths')\n",
    "\n",
    "    # Plot the 2D heatmap\n",
    "    ax2.contourf(X, Y, Z, levels=100, cmap='viridis')\n",
    "    ax2.set_title('Top-Down View (2D) - Optimization Paths')\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "\n",
    "    # Define specific colors for each optimizer\n",
    "    color_map = {\n",
    "        \"Regular\": \"red\",\n",
    "        \"Momentum\": \"blue\",\n",
    "        \"Nesterov\": \"green\",\n",
    "        \"AdaGrad\": \"orange\",\n",
    "        \"RMSProp\": \"purple\",\n",
    "        \"Adam\": \"black\"\n",
    "    }\n",
    "\n",
    "    # Plot the paths for each optimizer\n",
    "    for idx, (f, positions, z_values, name) in enumerate(optimizer_results):\n",
    "        positions_array = np.array(positions)\n",
    "        color = color_map.get(name, \"gray\")  # Default to gray if name not found\n",
    "        \n",
    "        # 3D Path\n",
    "        ax1.scatter(\n",
    "            positions_array[:, 0], positions_array[:, 1], z_values,\n",
    "            color=color, s=50, label=name\n",
    "        )\n",
    "        \n",
    "        # 2D Path\n",
    "        ax2.plot(\n",
    "            positions_array[:, 0],\n",
    "            positions_array[:, 1],\n",
    "            marker='o',\n",
    "            color=color,\n",
    "            markersize=5,\n",
    "            linestyle='--',\n",
    "            linewidth=2,\n",
    "            label=name\n",
    "        )\n",
    "\n",
    "    ax1.legend()  # Show legends to label different optimizer paths\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Method that takes plotting method and adds interactive elements\n",
    "def build_output(beta2_visible=False, optimizer_multiselect_options=[\"Regular\", \"Momentum\", \"Nesterov\", \"AdaGrad\", \"RMSProp\", \"Adam\"]):\n",
    "    lr_slider = widgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0,\n",
    "        max=2,\n",
    "        step=0.01,\n",
    "        description=\"Learning Rate\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    steps_slider = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=1,\n",
    "        max=30,\n",
    "        step=1,\n",
    "        description=\"Steps\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    func_dropdown = widgets.Dropdown(\n",
    "        description=\"Function\",\n",
    "        options=[\n",
    "            (\"Steep Valley Function\", steep_valley_function),\n",
    "            (\"simple Function\", simple_function),\n",
    "            (\"complex Function\", complex_function),\n",
    "            (\"L-shaped Valley Function\", l_shaped_valley_function),\n",
    "        ],\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    x_slider = widgets.IntSlider(\n",
    "        value=15,\n",
    "        min=-20,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        description=\"Starting X\",\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    y_slider = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=-20,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        description=\"Starting Y\",\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    beta1_slider = widgets.FloatSlider(\n",
    "        value=0.9,\n",
    "        min=0,\n",
    "        max=1,\n",
    "        step=0.01,\n",
    "        description=\"Beta1 Factor\",\n",
    "        readout_format='.3f',\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    beta2_slider = widgets.FloatSlider(\n",
    "        value=0.999,\n",
    "        min=0,\n",
    "        max=0.999999,\n",
    "        step=0.001,\n",
    "        description=\"Beta2 Factor\",\n",
    "        readout_format='.3f',\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    \n",
    "    beta2_slider.layout.visibility = 'visible' if beta2_visible else 'hidden'\n",
    "    \n",
    "    optimizer_multiselect = widgets.SelectMultiple(\n",
    "        description=\"Optimizer\",\n",
    "        value=[optimizer_multiselect_options[0]],\n",
    "        options=optimizer_multiselect_options,\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "\n",
    "    ui = widgets.VBox(\n",
    "        [\n",
    "            widgets.HBox([x_slider, y_slider]),\n",
    "            widgets.HBox([lr_slider, steps_slider]),\n",
    "            widgets.HBox([beta1_slider, beta2_slider]),\n",
    "            widgets.HBox([func_dropdown, optimizer_multiselect]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def update_plot(optimizers, function, steps, lr, beta1, beta2, initialX, initialY):\n",
    "        cur_pos = np.array([initialX, initialY])\n",
    "        results = optimize_multiple(\n",
    "            optimizers=optimizers,\n",
    "            function=function,\n",
    "            steps=steps,\n",
    "            lr=lr,\n",
    "            beta1=beta1,\n",
    "            beta2=beta2,\n",
    "            cur_pos=cur_pos,\n",
    "            print_res=False,\n",
    "        )\n",
    "        plot_optimization_path(results)\n",
    "\n",
    "    out = widgets.interactive_output(\n",
    "        update_plot,\n",
    "        {\n",
    "            \"optimizers\": optimizer_multiselect,\n",
    "            \"function\": func_dropdown,\n",
    "            \"steps\": steps_slider,\n",
    "            \"lr\": lr_slider,\n",
    "            \"beta1\": beta1_slider,\n",
    "            \"beta2\": beta2_slider,\n",
    "            \"initialX\": x_slider,\n",
    "            \"initialY\": y_slider,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Regular vs Momentum Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Hier werden die Functions für den Step und Gradient Descent der zwei ersten Optimizer gesetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Regular Gradient Descent Step and Function\n",
    "\n",
    "\n",
    "def regular_gradient_step(pos, lr, grad):\n",
    "    return pos - lr * grad(*pos)\n",
    "\n",
    "\n",
    "def regular_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    ") -> Tuple[Callable[[float, float], float], List[np.ndarray], List[float], str]:\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos = regular_gradient_step(cur_pos, lr, grad)\n",
    "        positions.append(cur_pos)\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Regular\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Momentum Gradient Descent Step and Function\n",
    "\n",
    "\n",
    "def momentum_step(pos, lr, beta, momentum, grad):\n",
    "    gradient = grad(*pos)\n",
    "    momentum = beta * momentum - lr * gradient\n",
    "    pos = pos + momentum\n",
    "\n",
    "    return momentum, pos\n",
    "\n",
    "\n",
    "def momentum_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    momentum = np.zeros(2)\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        momentum, cur_pos = momentum_step(cur_pos, lr, beta1, momentum, grad)\n",
    "        positions.append(cur_pos)\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Momentum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task / Visualize\n",
    "The Task in Section 1 is to play around with the Regular and Momentum optimizers and get comfortable using the visualizer tool.\n",
    "\n",
    "Try answering these Questions (for yourself):\n",
    "- How do the Optimizers behave for different `Starting X` and `Starting Y` values?\n",
    "- How does the Optimizers behaviour change, when the ``learning rate`` is adjusted?\n",
    "- What part of the Optimizers behaviour is influenced by the ``Beta1 Factor``?\n",
    "- How does the Optimizer behave in different ``Functions``?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac2045c706245b88cb5fed867bd1406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5779a3b7e3b41ec9ee3cd2cbf4fa97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "build_output(optimizer_multiselect_options=[\"Regular\", \"Momentum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Nesterov Accelerated Gradient (NAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Setup)\n",
    "Diese Section bedarf keines weiteren Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Your task is, to define the step function for the Nesterov Accelerated Gradient Descent (short, NAG).\n",
    "\n",
    "Tips:\n",
    "- Look closely at the mathematical definition\n",
    "- Orient yourself at the Momentum Optimizer's Step Function\n",
    "\n",
    "Nesterov Equation:\n",
    "$$ m \\leftarrow \\beta m - \\eta \\nabla_{\\theta} J(\\theta + \\beta m) $$\n",
    "$$ \\theta \\leftarrow \\theta + m $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov_step(pos, lr, beta, momentum, grad):\n",
    "    # TODO Implement the Nesterov Step function\n",
    "    # Important: the parameter grad is a Function giving you the value of the gradient at position x when calling grad(*x)\n",
    "    return pos, momentum\n",
    "\n",
    "\n",
    "def nesterov_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    momentum = np.zeros(2)  # Initial momentum vector\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos, momentum = nesterov_step(cur_pos, lr, beta1, momentum, grad)\n",
    "        positions.append(cur_pos.copy())  # Append the new position after update\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    # Use the generic plotting function to visualize the path\n",
    "    return (f, positions, z_values, \"Nesterov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e732f952b0946949550c762ebabf72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da872393d5d143b094feee0d92b69bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "build_output(optimizer_multiselect_options=['Nesterov'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: AdaGrad, RMSProp und Adam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_lars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
