{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from typing import Tuple, List, Callable, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Different Functions with different shapes\n",
    "def steep_valley_function():\n",
    "    def f(x, y):\n",
    "        # Define a function with a steeper gradient in the y-direction\n",
    "        return 0.5 * x**2 + 2 * y**2\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Compute the gradient of the function\n",
    "        grad_x = x  # Gradient with respect to x\n",
    "        grad_y = 4 * y  # Gradient with respect to y, steeper than x\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def l_shaped_valley_function():\n",
    "    def f(x, y):\n",
    "        # Creating an L-shaped valley with a sharp turn\n",
    "        # Define the valley with two different linear regions\n",
    "        # Horizontal part (x > y), Vertical part (x <= y)\n",
    "        return np.where(x > y, (y + 20) ** 2 + x, (x + 20) ** 2 + y)\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Gradient of the L-shaped function, changing at x = y\n",
    "        # Horizontal part gradient\n",
    "        grad_x = np.where(x > y, 1, 2 * (x + 20))\n",
    "        grad_y = np.where(x > y, 2 * (y + 20), 1)\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-50, 20, 400)\n",
    "    y = np.linspace(-50, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def complex_function():\n",
    "    def f(x, y):\n",
    "        # Scale the coefficients to increase the depth and curvature differences\n",
    "        return (\n",
    "            0.3 * (x - 10) ** 2\n",
    "            + 0.2 * (y - 10) ** 2\n",
    "            + 1 * (x + 10) ** 2\n",
    "            + 1 * (y + 10) ** 2\n",
    "        )\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Adjusted gradient for the scaled function\n",
    "        grad_x = 0.6 * (x - 10) + 2 * (\n",
    "            x + 10\n",
    "        )  # Increased coefficients for the gradients\n",
    "        grad_y = 0.4 * (y - 10) + 2 * (y + 10)\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def simple_function():\n",
    "    def f(x, y):\n",
    "        return x**2 + y**2  # Z as a function of X and Y\n",
    "\n",
    "    def grad(x, y):\n",
    "        grad_x = 2 * x\n",
    "        grad_y = 2 * y\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 100)\n",
    "    y = np.linspace(-20, 20, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining General optimize function to standardize paramters and return\n",
    "def optimize(\n",
    "    # parent method to call all other optimization methods\n",
    "    optimizer_type=\"Regular\",\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    ") -> Tuple[Callable[[float, float], float], List[np.ndarray], List[float], str]:\n",
    "\n",
    "    # Choose and run the optimizer\n",
    "    # they always return this way: X Y Z positions,\n",
    "    if optimizer_type == \"Regular\":\n",
    "        return regular_gradient_descent(function, steps, lr, cur_pos, print_res)\n",
    "    elif optimizer_type == \"Momentum\":\n",
    "        return momentum_gradient_descent(function, steps, lr, beta1, cur_pos, print_res)\n",
    "    elif optimizer_type == \"Nesterov\":\n",
    "        return nesterov_gradient_descent(function, steps, lr, beta1, cur_pos, print_res)\n",
    "    elif optimizer_type == \"AdaGrad\":\n",
    "        return adagrad_gradient_descent(function, steps, lr, cur_pos, print_res)\n",
    "    elif optimizer_type == \"RMSProp\":\n",
    "        return rmsprop_gradient_descent(function, steps, lr, beta1, cur_pos, print_res)\n",
    "    elif optimizer_type == \"Adam\":\n",
    "        return adam_gradient_descent(\n",
    "            function, steps, lr, beta1, beta2, cur_pos, print_res\n",
    "        )\n",
    "\n",
    "\n",
    "def optimize_multiple(\n",
    "    optimizers=[\"Regular\", \"momentum\"],\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    results = []\n",
    "    for optimizer in optimizers:\n",
    "        results.append(\n",
    "            optimize(optimizer, function, steps, lr, beta1, beta2, cur_pos, print_res)\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Function (not Interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Function that takes a list of results and Returns a Plot (not yet interactive)\n",
    "def plot_optimization_path(optimizer_results):\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax2 = fig.add_subplot(1, 2, 2)  # 2D plot for top-down heatmap view\n",
    "\n",
    "    # Assuming all optimizers use the same function landscape\n",
    "    f = optimizer_results[0][0]  # Extract function handle from first result\n",
    "    x_range = np.linspace(-20, 20, 400)\n",
    "    y_range = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = f(X, Y)\n",
    "\n",
    "    # Plot the 3D surface\n",
    "    ax1.plot_surface(X, Y, Z, alpha=0.5, cmap='viridis', edgecolor='none')\n",
    "    ax1.set_title('3D View - Optimization Paths')\n",
    "\n",
    "    # Plot the 2D heatmap\n",
    "    ax2.contourf(X, Y, Z, levels=100, cmap='viridis')\n",
    "    ax2.set_title('Top-Down View (2D) - Optimization Paths')\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "\n",
    "    # Define specific colors for each optimizer\n",
    "    color_map = {\n",
    "        \"Regular\": \"red\",\n",
    "        \"Momentum\": \"blue\",\n",
    "        \"Nesterov\": \"green\",\n",
    "        \"AdaGrad\": \"orange\",\n",
    "        \"RMSProp\": \"purple\",\n",
    "        \"Adam\": \"black\"\n",
    "    }\n",
    "\n",
    "    # Plot the paths for each optimizer\n",
    "    for idx, (f, positions, z_values, name) in enumerate(optimizer_results):\n",
    "        positions_array = np.array(positions)\n",
    "        color = color_map.get(name, \"gray\")  # Default to gray if name not found\n",
    "        \n",
    "        # 3D Path\n",
    "        ax1.scatter(\n",
    "            positions_array[:, 0], positions_array[:, 1], z_values,\n",
    "            color=color, s=50, label=name\n",
    "        )\n",
    "        \n",
    "        # 2D Path\n",
    "        ax2.plot(\n",
    "            positions_array[:, 0],\n",
    "            positions_array[:, 1],\n",
    "            marker='o',\n",
    "            color=color,\n",
    "            markersize=5,\n",
    "            linestyle='--',\n",
    "            linewidth=2,\n",
    "            label=name\n",
    "        )\n",
    "\n",
    "    ax1.legend()  # Show legends to label different optimizer paths\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Gradient Descent Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Gradient\n",
    "def regular_gradient_step(pos, lr, grad):\n",
    "    return pos - lr * grad(*pos)\n",
    "\n",
    "\n",
    "def regular_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    ") -> Tuple[Callable[[float, float], float], List[np.ndarray], List[float], str]:\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos = regular_gradient_step(cur_pos, lr, grad)\n",
    "        positions.append(cur_pos)\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Regular\")\n",
    "\n",
    "\n",
    "def momentum_step(pos, lr, beta, momentum, grad):\n",
    "    gradient = grad(*pos)\n",
    "    momentum = beta * momentum - lr * gradient\n",
    "    pos = pos + momentum\n",
    "\n",
    "    return momentum, pos\n",
    "\n",
    "\n",
    "def momentum_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    momentum = np.zeros(2)\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        momentum, cur_pos = momentum_step(cur_pos, lr, beta1, momentum, grad)\n",
    "        positions.append(cur_pos)\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Momentum\")\n",
    "\n",
    "\n",
    "def nesterov_step(pos, lr, beta, momentum, grad):\n",
    "    lookahead_pos = (\n",
    "        pos + beta * momentum\n",
    "    )  # Use the gradient slightly ahead in the direction of the momentum\n",
    "    gradient = grad(*lookahead_pos)\n",
    "    momentum = beta * momentum - lr * gradient\n",
    "    pos = pos + momentum\n",
    "\n",
    "    return momentum, pos\n",
    "\n",
    "\n",
    "def nesterov_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    momentum = np.zeros(2)  # Initial momentum vector\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        momentum, cur_pos = nesterov_step(cur_pos, lr, beta1, momentum, grad)\n",
    "        positions.append(cur_pos.copy())  # Append the new position after update\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    # Use the generic plotting function to visualize the path\n",
    "    return (f, positions, z_values, \"Nesterov\")\n",
    "\n",
    "\n",
    "def adagrad_step(pos, lr, grad, s):\n",
    "    epsilon = 10 ** (-10)  # set to different value if needed\n",
    "    gradient = grad(*pos)\n",
    "    s += gradient**2  # Accumulate squared gradients\n",
    "    adjusted_lr = lr / (np.sqrt(s) + epsilon)\n",
    "    pos = pos - adjusted_lr * gradient\n",
    "    return pos, s\n",
    "\n",
    "\n",
    "def adagrad_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    s = np.zeros(2)  # Initialize sum of squares of gradients\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos, s = adagrad_step(cur_pos, lr, grad, s)\n",
    "        positions.append(cur_pos.copy())  # Ensure a deep copy for immutability in list\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"AdaGrad\")\n",
    "\n",
    "\n",
    "def rmsprop_step(pos, lr, grad, s, beta=0.9, epsilon=1e-8):\n",
    "    gradient = grad(*pos)\n",
    "    s = beta * s + (1 - beta) * gradient * gradient\n",
    "    pos = pos - (lr * gradient / (np.sqrt(s + epsilon)))\n",
    "    return pos, s\n",
    "\n",
    "    # # alternative function with adjusted learning rate\n",
    "    # gradient = grad(*pos)\n",
    "    # s = beta * s + (1 - beta) * gradient ** 2  # Update accumulation of squared gradients\n",
    "    # adjusted_lr = lr / (np.sqrt(s) + epsilon)  # Adjust learning rate\n",
    "    # pos = pos - adjusted_lr * gradient  # Update position\n",
    "    # return pos, s\n",
    "\n",
    "\n",
    "def rmsprop_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    s = np.zeros_like(cur_pos)  # Initialize the RMS accumulation variable\n",
    "    for i in range(steps):\n",
    "        cur_pos, s = rmsprop_step(\n",
    "            cur_pos, lr, grad, s, beta1\n",
    "        )  # Update position and RMS\n",
    "        positions.append(cur_pos.copy())  # Ensure a deep copy for immutability in list\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"RMSProp\")\n",
    "\n",
    "\n",
    "def adam_step(pos, lr, grad, m, v, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    gradient = grad(*pos)\n",
    "    m = beta1 * m + (1 - beta1) * gradient  # Update biased first moment estimate\n",
    "    v = beta2 * v + (1 - beta2) * gradient**2  # Update biased second moment estimate\n",
    "    m_hat = m / (1 - beta1**t)  # Compute bias-corrected first moment estimate\n",
    "    v_hat = v / (1 - beta2**t)  # Compute bias-corrected second moment estimate\n",
    "    pos = pos - lr * m_hat / (np.sqrt(v_hat) + epsilon)  # Update parameters\n",
    "    return pos, m, v\n",
    "\n",
    "\n",
    "def adam_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.99,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    m = np.zeros_like(cur_pos)  # Initialize first moment vector\n",
    "    v = np.zeros_like(cur_pos)  # Initialize second moment vector\n",
    "    t = 0  # Initialize timestep\n",
    "\n",
    "    positions = [cur_pos.copy()]  # Store positions for plotting\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        t += 1  # Increment time step\n",
    "        cur_pos, m, v = adam_step(\n",
    "            cur_pos, lr, grad, m, v, t, beta1, beta2\n",
    "        )  # Adam optimization step\n",
    "        positions.append(cur_pos.copy())  # Ensure a deep copy for immutability in list\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    # Use the plot function to visualize the optimization path\n",
    "    return (f, positions, z_values, \"Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea570c064eb4d71aa2854594f05f931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aff8f34500e4d7dbe100037f4a528e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build Method that takes plotting method and adds interactive elements\n",
    "def build_output():\n",
    "    lr_slider = widgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0,\n",
    "        max=2,\n",
    "        step=0.01,\n",
    "        description=\"Learning Rate\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    steps_slider = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=1,\n",
    "        max=30,\n",
    "        step=1,\n",
    "        description=\"Steps\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    func_dropdown = widgets.Dropdown(\n",
    "        description=\"Function\",\n",
    "        options=[\n",
    "            (\"Steep Valley Function\", steep_valley_function),\n",
    "            (\"simple Function\", simple_function),\n",
    "            (\"complex Function\", complex_function),\n",
    "            (\"L-shaped Valley Function\", l_shaped_valley_function),\n",
    "        ],\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    x_slider = widgets.IntSlider(\n",
    "        value=15,\n",
    "        min=-20,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        description=\"Starting X\",\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    y_slider = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=-20,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        description=\"Starting Y\",\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    beta1_slider = widgets.FloatSlider(\n",
    "        value=0.9,\n",
    "        min=0,\n",
    "        max=1,\n",
    "        step=0.01,\n",
    "        description=\"Beta1 Factor\",\n",
    "        readout_format='.3f',\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    beta2_slider = widgets.FloatSlider(\n",
    "        value=0.999,\n",
    "        min=0,\n",
    "        max=0.999999,\n",
    "        step=0.001,\n",
    "        description=\"Beta2 Factor\",\n",
    "        readout_format='.3f',\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    optimizer_multiselect = widgets.SelectMultiple(\n",
    "        description=\"Optimizer\",\n",
    "        value=[\"Regular\", \"Momentum\"],\n",
    "        options=[\"Regular\", \"Momentum\", \"Nesterov\", \"AdaGrad\", \"RMSProp\", \"Adam\"],\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "\n",
    "    ui = widgets.VBox(\n",
    "        [\n",
    "            widgets.HBox([x_slider, y_slider]),\n",
    "            widgets.HBox([lr_slider, steps_slider]),\n",
    "            widgets.HBox([beta1_slider, beta2_slider]),\n",
    "            widgets.HBox([func_dropdown, optimizer_multiselect]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def update_plot(optimizers, function, steps, lr, beta1, beta2, initialX, initialY):\n",
    "        cur_pos = np.array([initialX, initialY])\n",
    "        results = optimize_multiple(\n",
    "            optimizers=optimizers,\n",
    "            function=function,\n",
    "            steps=steps,\n",
    "            lr=lr,\n",
    "            beta1=beta1,\n",
    "            beta2=beta2,\n",
    "            cur_pos=cur_pos,\n",
    "            print_res=False,\n",
    "        )\n",
    "        plot_optimization_path(results)\n",
    "\n",
    "    out = widgets.interactive_output(\n",
    "        update_plot,\n",
    "        {\n",
    "            \"optimizers\": optimizer_multiselect,\n",
    "            \"function\": func_dropdown,\n",
    "            \"steps\": steps_slider,\n",
    "            \"lr\": lr_slider,\n",
    "            \"beta1\": beta1_slider,\n",
    "            \"beta2\": beta2_slider,\n",
    "            \"initialX\": x_slider,\n",
    "            \"initialY\": y_slider,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    display(ui, out)\n",
    "\n",
    "\n",
    "build_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
