{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_initialize():\n",
    "    x = randint(-20, 20)\n",
    "    y = randint(-20, 20)\n",
    "    return np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "def steep_valley_function():\n",
    "    def f(x, y):\n",
    "        # Define a function with a steeper gradient in the y-direction\n",
    "        return 0.5 * x**2 + 2 * y**2\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Compute the gradient of the function\n",
    "        grad_x = x  # Gradient with respect to x\n",
    "        grad_y = 4 * y  # Gradient with respect to y, steeper than x\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def l_shaped_valley_function():\n",
    "    def f(x, y):\n",
    "        # Creating an L-shaped valley with a sharp turn\n",
    "        # Define the valley with two different linear regions\n",
    "        # Horizontal part (x > y), Vertical part (x <= y)\n",
    "        return np.where(x > y, (y + 20) ** 2 + x, (x + 20) ** 2 + y)\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Gradient of the L-shaped function, changing at x = y\n",
    "        # Horizontal part gradient\n",
    "        grad_x = np.where(x > y, 1, 2 * (x + 20))\n",
    "        grad_y = np.where(x > y, 2 * (y + 20), 1)\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-50, 20, 400)\n",
    "    y = np.linspace(-50, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def complex_function():\n",
    "    def f(x, y):\n",
    "        # Scale the coefficients to increase the depth and curvature differences\n",
    "        return (\n",
    "            0.3 * (x - 10) ** 2\n",
    "            + 0.2 * (y - 10) ** 2\n",
    "            + 1 * (x + 10) ** 2\n",
    "            + 1 * (y + 10) ** 2\n",
    "        )\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Adjusted gradient for the scaled function\n",
    "        grad_x = 0.6 * (x - 10) + 2 * (\n",
    "            x + 10\n",
    "        )  # Increased coefficients for the gradients\n",
    "        grad_y = 0.4 * (y - 10) + 2 * (y + 10)\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def simple_function():\n",
    "    def f(x, y):\n",
    "        return x**2 + y**2  # Z as a function of X and Y\n",
    "\n",
    "    def grad(x, y):\n",
    "        grad_x = 2 * x\n",
    "        grad_y = 2 * y\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 100)\n",
    "    y = np.linspace(-20, 20, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def build_output(method=\"gradient\"):\n",
    "\n",
    "    lr_slider = widgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0,\n",
    "        max=2,\n",
    "        step=0.01,\n",
    "        description=\"Learning Rate\",\n",
    "        style={\n",
    "            \"description_width\": \"initial\"\n",
    "        },  # Ensure description does not get cut off\n",
    "        layout=widgets.Layout(width=\"100%\"),  # Full width of the container\n",
    "    )\n",
    "\n",
    "    steps_slider = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=1,\n",
    "        max=30,\n",
    "        step=1,\n",
    "        description=\"Steps\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "\n",
    "    func_dropdown = widgets.Dropdown(\n",
    "        description=\"Function\",\n",
    "        options=[\n",
    "            (\"simple Function\", simple_function),\n",
    "            (\"complex Function\", complex_function),\n",
    "            (\"L-shaped Valley Function\", l_shaped_valley_function),\n",
    "            (\"Steep Valley Function\", steep_valley_function),\n",
    "        ],  # Use a list of tuples for clarity\n",
    "        layout=widgets.Layout(\n",
    "            width=\"50%\"\n",
    "        ),  # Adjust as needed based on UI considerations\n",
    "    )\n",
    "\n",
    "    x_slider = widgets.IntSlider(\n",
    "        value=15,\n",
    "        min=-20,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        description=\"Starting X\",\n",
    "        layout=widgets.Layout(width=\"50%\"),\n",
    "    )\n",
    "\n",
    "    y_slider = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=-20,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        description=\"Starting Y\",\n",
    "        layout=widgets.Layout(width=\"50%\"),\n",
    "    )\n",
    "\n",
    "    beta_slider = widgets.FloatSlider(\n",
    "        value=0.9,\n",
    "        min=-0,\n",
    "        max=1,\n",
    "        step=0.01,\n",
    "        description=\"Beta Factor\",\n",
    "        layout=widgets.Layout(width=\"50%\"),\n",
    "    )\n",
    "\n",
    "    # Create a horizontal box to hold the widgets\n",
    "\n",
    "    match method:\n",
    "        case \"gradient\":\n",
    "            ui = widgets.HBox(\n",
    "                [x_slider, y_slider, lr_slider, steps_slider, func_dropdown],\n",
    "                layout=widgets.Layout(flex_flow=\"row wrap\"),\n",
    "            )  # Wrap the content if not enough space\n",
    "\n",
    "            out = widgets.interactive_output(\n",
    "                regular_gradient_descent,\n",
    "                {\n",
    "                    \"initialX\": x_slider,\n",
    "                    \"initialY\": y_slider,\n",
    "                    \"lr\": lr_slider,\n",
    "                    \"steps\": steps_slider,\n",
    "                    \"function\": func_dropdown,\n",
    "                    \"print_res\": widgets.fixed(False),\n",
    "                    \"initialRandom\": widgets.fixed(False),\n",
    "                },\n",
    "            )\n",
    "        case \"momentum\":\n",
    "            ui = widgets.HBox(\n",
    "                [\n",
    "                    x_slider,\n",
    "                    y_slider,\n",
    "                    lr_slider,\n",
    "                    steps_slider,\n",
    "                    beta_slider,\n",
    "                    func_dropdown,\n",
    "                ],\n",
    "                layout=widgets.Layout(flex_flow=\"row wrap\"),\n",
    "            )  # Wrap the content if not enough space\n",
    "            out = widgets.interactive_output(\n",
    "                momentum_gradient_descent,\n",
    "                {\n",
    "                    \"initialX\": x_slider,\n",
    "                    \"initialY\": y_slider,\n",
    "                    \"lr\": lr_slider,\n",
    "                    \"steps\": steps_slider,\n",
    "                    \"function\": func_dropdown,\n",
    "                    \"beta\": beta_slider,\n",
    "                    \"print_res\": widgets.fixed(False),\n",
    "                    \"initialRandom\": widgets.fixed(False),\n",
    "                },\n",
    "            )\n",
    "        case \"nesterov\":\n",
    "            ui = widgets.HBox(\n",
    "                [\n",
    "                    x_slider,\n",
    "                    y_slider,\n",
    "                    lr_slider,\n",
    "                    steps_slider,\n",
    "                    beta_slider,\n",
    "                    func_dropdown,\n",
    "                ],\n",
    "                layout=widgets.Layout(flex_flow=\"row wrap\"),\n",
    "            )  # Wrap the content if not enough space\n",
    "            out = widgets.interactive_output(\n",
    "                nesterov_gradient_descent,\n",
    "                {\n",
    "                    \"initialX\": x_slider,\n",
    "                    \"initialY\": y_slider,\n",
    "                    \"lr\": lr_slider,\n",
    "                    \"steps\": steps_slider,\n",
    "                    \"function\": func_dropdown,\n",
    "                    \"beta\": beta_slider,\n",
    "                    \"print_res\": widgets.fixed(False),\n",
    "                    \"initialRandom\": widgets.fixed(False),\n",
    "                },\n",
    "            )\n",
    "        case \"adagrad\":\n",
    "            ui = widgets.HBox(\n",
    "                [x_slider, y_slider, lr_slider, steps_slider, func_dropdown],\n",
    "                layout=widgets.Layout(flex_flow=\"row wrap\"),\n",
    "            )  # Wrap the content if not enough space\n",
    "            out = widgets.interactive_output(\n",
    "                adagrad_gradient_descent,\n",
    "                {\n",
    "                    \"initialX\": x_slider,\n",
    "                    \"initialY\": y_slider,\n",
    "                    \"lr\": lr_slider,\n",
    "                    \"steps\": steps_slider,\n",
    "                    \"function\": func_dropdown,\n",
    "                    \"print_res\": widgets.fixed(False),\n",
    "                    \"initialRandom\": widgets.fixed(False),\n",
    "                },\n",
    "            )\n",
    "        case \"rmsprop\":\n",
    "            ui = widgets.HBox(\n",
    "                [\n",
    "                    x_slider,\n",
    "                    y_slider,\n",
    "                    lr_slider,\n",
    "                    steps_slider,\n",
    "                    beta_slider,\n",
    "                    func_dropdown,\n",
    "                ],\n",
    "                layout=widgets.Layout(flex_flow=\"row wrap\"),\n",
    "            )  # Arrange widgets in a row, wrap if space is insufficient\n",
    "\n",
    "            out = widgets.interactive_output(\n",
    "                rmsprop_gradient_descent,\n",
    "                {\n",
    "                    \"initialX\": x_slider,\n",
    "                    \"initialY\": y_slider,\n",
    "                    \"lr\": lr_slider,\n",
    "                    \"steps\": steps_slider,\n",
    "                    \"beta\": beta_slider,\n",
    "                    \"function\": func_dropdown,\n",
    "                    \"print_res\": widgets.fixed(False),\n",
    "                    \"initialRandom\": widgets.fixed(False),\n",
    "                },\n",
    "            )\n",
    "        case \"adam\":\n",
    "            ui = widgets.HBox(\n",
    "                [x_slider, y_slider, lr_slider, steps_slider, func_dropdown],\n",
    "                layout=widgets.Layout(flex_flow=\"row wrap\"),\n",
    "            )\n",
    "            out = widgets.interactive_output(\n",
    "                adam_gradient_descent,\n",
    "                {\n",
    "                    \"initialX\": x_slider,\n",
    "                    \"initialY\": y_slider,\n",
    "                    \"lr\": lr_slider,\n",
    "                    \"steps\": steps_slider,\n",
    "                    \"function\": func_dropdown,\n",
    "                    \"print_res\": widgets.fixed(False),\n",
    "                    \"initialRandom\": widgets.fixed(False),\n",
    "                },\n",
    "            )\n",
    "\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optimization_path(X, Y, Z, positions, function, function_name):\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "    ax2 = fig.add_subplot(1, 2, 2)  # 2D plot for top-down heatmap view\n",
    "\n",
    "    # Plot the 3D surface\n",
    "    ax1.plot_surface(X, Y, Z, alpha=0.5, cmap=\"viridis\", edgecolor=\"none\")\n",
    "    ax1.set_title(\"3D View - \" + function_name)\n",
    "\n",
    "    # Convert positions to an array for easier manipulation\n",
    "    positions_array = np.array(positions)\n",
    "    # Calculate Z values for each X, Y position\n",
    "    z_values = np.array([function(pos[0], pos[1]) for pos in positions_array])\n",
    "\n",
    "    # Plotting the path on the 3D plot\n",
    "    ax1.scatter(\n",
    "        positions_array[:, 0], positions_array[:, 1], z_values, color=\"red\", s=50\n",
    "    )\n",
    "\n",
    "    # Create a contour plot for the 2D view\n",
    "    ax2.contourf(X, Y, Z, levels=100, cmap=\"viridis\")\n",
    "    ax2.plot(\n",
    "        positions_array[:, 0],\n",
    "        positions_array[:, 1],\n",
    "        marker=\"o\",\n",
    "        color=\"red\",\n",
    "        markersize=5,\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    ax2.set_title(\"Top-Down View (2D) - \" + function_name)\n",
    "    ax2.set_xlabel(\"X\")\n",
    "    ax2.set_ylabel(\"Y\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6147abe15bf0490aac6516325193d1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='50%'), max=20, min=-20), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b1b0c25a034da9b802174b38e32228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def regular_gradient_step(pos, lr, grad):\n",
    "    return pos - lr * grad(*pos)\n",
    "\n",
    "\n",
    "def regular_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    print_res=False,\n",
    "    initialRandom=False,\n",
    "    initialX=0,\n",
    "    initialY=0,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    cur_pos = np.array([initialX, initialY])\n",
    "    positions = [cur_pos]\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos = regular_gradient_step(cur_pos, lr, grad)\n",
    "        positions.append(cur_pos)\n",
    "        if print_res:\n",
    "            cur_z = f(cur_pos[0], cur_pos[1])\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    plot_optimization_path(X, Y, Z, positions, f, \"Regular Gradient Descent\")\n",
    "\n",
    "\n",
    "build_output(method=\"gradient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "m \\leftarrow \\beta m - \\eta \\nabla_{\\theta} J(\\theta) \\\\\n",
    "\\theta \\leftarrow \\theta + m\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9575cf337704a4c84d4a0752c1dc46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='50%'), max=20, min=-20), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3529351a4a046f2b2ba7913b1f9f00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def momentum_step(pos, lr, beta, momentum, grad):\n",
    "    gradient = grad(*pos)\n",
    "    momentum = beta * momentum - lr * gradient\n",
    "    pos = pos + momentum\n",
    "\n",
    "    return momentum, pos\n",
    "\n",
    "\n",
    "def momentum_gradient_descent(\n",
    "    function=simple_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta=0.1,\n",
    "    print_res=False,\n",
    "    initialRandom=True,\n",
    "    initialX=10,\n",
    "    initialY=10,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    cur_pos = random_initialize() if initialRandom else np.array([initialX, initialY])\n",
    "    momentum = np.zeros(2)\n",
    "    positions = [cur_pos]\n",
    "\n",
    "    for i in range(steps):\n",
    "        momentum, cur_pos = momentum_step(cur_pos, lr, beta, momentum, grad)\n",
    "        positions.append(cur_pos)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}\")\n",
    "\n",
    "    plot_optimization_path(X, Y, Z, positions, f, \"Momentum Gradient Descent\")\n",
    "\n",
    "\n",
    "build_output(method=\"momentum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't yet work with new plot function\n",
    "# print(\"Regular Gradient-Descent\")\n",
    "# regular_gradient_descent(steps=10, initialRandom=False, initialX=25, print_res=False)\n",
    "# print(\"Momentum Optimization\")\n",
    "# momentum_gradient_descent(steps=10,beta=0.5, initialRandom=False, initialX=25, print_res=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ m \\leftarrow \\beta m - \\eta \\nabla_{\\theta} J(\\theta + \\beta m) $$\n",
    "$$ \\theta \\leftarrow \\theta + m $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4588200aabb94ce6ad7c497608108a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='50%'), max=20, min=-20), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26ddb09c8fd487b89f6c8731248971b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def nesterov_step(pos, lr, beta, momentum, grad):\n",
    "    lookahead_pos = (\n",
    "        pos + beta * momentum\n",
    "    )  # Use the gradient slightly ahead in the direction of the momentum\n",
    "    gradient = grad(*lookahead_pos)\n",
    "    momentum = beta * momentum - lr * gradient\n",
    "    pos = pos + momentum\n",
    "\n",
    "    return momentum, pos\n",
    "\n",
    "\n",
    "def nesterov_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta=0.1,\n",
    "    print_res=False,\n",
    "    initialRandom=False,\n",
    "    initialX=10,\n",
    "    initialY=10,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    if initialRandom:\n",
    "        cur_pos = random_initialize()\n",
    "    else:\n",
    "        cur_pos = np.array([initialX, initialY])  # Starting position as a 2D vector\n",
    "\n",
    "    momentum = np.zeros(2)  # Initial momentum vector\n",
    "    positions = [cur_pos.copy()]  # List to store positions for plotting\n",
    "\n",
    "    for i in range(steps):\n",
    "        momentum, cur_pos = nesterov_step(cur_pos, lr, beta, momentum, grad)\n",
    "        positions.append(cur_pos.copy())  # Append the new position after update\n",
    "        if print_res:\n",
    "            cur_z = f(cur_pos[0], cur_pos[1])\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    # Use the generic plotting function to visualize the path\n",
    "    plot_optimization_path(X, Y, Z, positions, f, \"Nesterov Gradient Descent\")\n",
    "\n",
    "\n",
    "build_output(method=\"nesterov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{s} \\leftarrow \\mathbf{s} + \\nabla_{\\theta} J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) $$\n",
    "$$ \\theta \\leftarrow \\theta - \\eta  \\nabla_{\\theta} J(\\theta) \\oslash \\sqrt{\\mathbf{s} + \\epsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9bba7e186c467eb9710d10273cc6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='50%'), max=20, min=-20), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef225d2c4694f1894f6dac264d08d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def adagrad_step(pos, lr, grad, s):\n",
    "    epsilon = 10 ** (-10)  # set to different value if needed\n",
    "    gradient = grad(*pos)\n",
    "    s = s + gradient**2  # Accumulate squared gradients\n",
    "    adjusted_lr = lr / (np.sqrt(s) + epsilon)\n",
    "    pos = pos - adjusted_lr * gradient\n",
    "\n",
    "    return pos, s\n",
    "\n",
    "\n",
    "def adagrad_gradient_descent(\n",
    "    function=simple_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    print_res=False,\n",
    "    initialRandom=True,\n",
    "    initialX=10,\n",
    "    initialY=10,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    if initialRandom:\n",
    "        cur_pos = random_initialize()\n",
    "    else:\n",
    "        cur_pos = np.array([initialX, initialY])  # Starting position as a 2D vector\n",
    "\n",
    "    s = np.zeros(2)  # Initialize sum of squares of gradients\n",
    "    positions = [cur_pos.copy()]  # Store positions for plotting\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos, s = adagrad_step(cur_pos, lr, grad, s)\n",
    "        positions.append(cur_pos.copy())  # Append the new position after update\n",
    "        if print_res:\n",
    "            cur_z = f(cur_pos[0], cur_pos[1])\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    # Use the generic plotting function to visualize the path\n",
    "    plot_optimization_path(X, Y, Z, positions, f, \"AdaGrad Gradient Descent\")\n",
    "\n",
    "\n",
    "build_output(method=\"adagrad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{s} \\leftarrow \\beta \\mathbf{s} + (1-\\beta) \\nabla_{\\theta} J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) $$\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta) \\oslash \\sqrt{\\mathbf{s} + \\epsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5a801babce470f818f4382e3662c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='50%'), max=20, min=-20), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f7771207434cc6be0175b198b4e384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rmsprop_step(pos, lr, grad, s, beta=0.9, epsilon=1e-8):\n",
    "    # Compute the gradient at the current position\n",
    "    gradient = grad(*pos)\n",
    "\n",
    "    # Update the moving average of the squared gradients\n",
    "    s = beta * s + (1 - beta) * gradient * gradient  # gradient**2 also possible\n",
    "\n",
    "    pos = pos - (lr * gradient / (np.sqrt(s + epsilon)))\n",
    "\n",
    "    return pos, s\n",
    "\n",
    "    # # alternative function with adjusted learning rate\n",
    "    # gradient = grad(*pos)\n",
    "    # s = beta * s + (1 - beta) * gradient ** 2  # Update accumulation of squared gradients\n",
    "    # adjusted_lr = lr / (np.sqrt(s) + epsilon)  # Adjust learning rate\n",
    "    # pos = pos - adjusted_lr * gradient  # Update position\n",
    "    # return pos, s\n",
    "\n",
    "\n",
    "def rmsprop_gradient_descent(\n",
    "    function=simple_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta=0.9,\n",
    "    print_res=False,\n",
    "    initialRandom=True,\n",
    "    initialX=10,\n",
    "    initialY=10,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()  # Load the function's landscape and gradient\n",
    "    if initialRandom:\n",
    "        # If random initialization is specified\n",
    "        cur_pos = np.array([random.randint(-20, 20), random.randint(-20, 20)])\n",
    "    else:\n",
    "        cur_pos = np.array([initialX, initialY])  # Use the specified initial position\n",
    "\n",
    "    s = np.zeros_like(cur_pos)  # Initialize the RMS accumulation variable\n",
    "    positions = [cur_pos.copy()]  # List to collect all positions for visualization\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos, s = rmsprop_step(cur_pos, lr, grad, s, beta)  # Update position and RMS\n",
    "        positions.append(cur_pos.copy())  # Append the new position for plotting\n",
    "        if print_res:\n",
    "            cur_z = f(*cur_pos)\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    # Call the plot function at the end of all iterations\n",
    "    plot_optimization_path(X, Y, Z, positions, f, \"RMSProp Gradient Descent\")\n",
    "\n",
    "\n",
    "build_output(method=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{m} \\leftarrow \\beta_1 \\mathbf{m} + (1 - \\beta_1) \\nabla_{\\theta} J(\\theta) $$\n",
    "$$ \\mathbf{s} \\leftarrow \\beta_2 \\mathbf{s} + (1 - \\beta_2) \\nabla_{\\theta} J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) $$\n",
    "$$ \\widehat{\\mathbf{m}} \\leftarrow \\frac{\\mathbf{m}}{1 - \\beta_1^t}, \\quad \\widehat{\\mathbf{s}} \\leftarrow \\frac{\\mathbf{s}}{1 - \\beta_2^t} $$\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\widehat{\\mathbf{m}} \\oslash \\sqrt{\\widehat{\\mathbf{s}} + \\epsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabdf51f34e749afa042af61c7d2af6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='50%'), max=20, min=-20), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df41ffe47b414042a2ec9675bb660b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def adam_step(pos, lr, grad, m, v, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    gradient = grad(*pos)\n",
    "    m = beta1 * m + (1 - beta1) * gradient  # Update biased first moment estimate\n",
    "    v = beta2 * v + (1 - beta2) * gradient**2  # Update biased second moment estimate\n",
    "    m_hat = m / (1 - beta1**t)  # Compute bias-corrected first moment estimate\n",
    "    v_hat = v / (1 - beta2**t)  # Compute bias-corrected second moment estimate\n",
    "    pos = pos - lr * m_hat / (np.sqrt(v_hat) + epsilon)  # Update parameters\n",
    "    return pos, m, v\n",
    "\n",
    "\n",
    "def adam_gradient_descent(\n",
    "    function=simple_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    print_res=False,\n",
    "    initialRandom=True,\n",
    "    initialX=10,\n",
    "    initialY=10,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    if initialRandom:\n",
    "        cur_pos = np.array([np.random.randint(-20, 20), np.random.randint(-20, 20)])\n",
    "    else:\n",
    "        cur_pos = np.array([initialX, initialY])\n",
    "\n",
    "    m = np.zeros_like(cur_pos)  # Initialize first moment vector\n",
    "    v = np.zeros_like(cur_pos)  # Initialize second moment vector\n",
    "    t = 0  # Initialize timestep\n",
    "\n",
    "    positions = [cur_pos.copy()]  # Store positions for plotting\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        t += 1  # Increment time step\n",
    "        cur_pos, m, v = adam_step(cur_pos, lr, grad, m, v, t)  # Adam optimization step\n",
    "        positions.append(cur_pos.copy())\n",
    "        if print_res:\n",
    "            cur_z = f(*cur_pos)\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    # Use the plot function to visualize the optimization path\n",
    "    plot_optimization_path(X, Y, Z, positions, f, \"Adam Gradient Descent\")\n",
    "\n",
    "\n",
    "build_output(method=\"adam\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
