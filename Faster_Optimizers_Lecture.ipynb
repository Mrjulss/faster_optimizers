{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Jupyter Notebook to Learn about faster Optimizers and visualize them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "1. Beginnen Sie damit, den ``import`` Block auszuführen, um zu sehen, ob die benötigten Pakete installiert sind. Installieren Sie fehlende Pakete, bis Sie den Block erfolgreich ausführen können.\n",
    "2. Gehen Sie durch die Sektionen. Führen Sie die folgenden Schritte für jeden Abschnitt aus\n",
    "    1. Führen Sie den `Setup`-Block aus. Dadurch werden alle notwendigen Funktionen für den Abschnitt erstellt.\n",
    "    2. Bearbeite den Block `Task`.\n",
    "    3. Visualisieren und Testen mit dem `Visualize`-Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from typing import Tuple, List, Callable, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionsdefinitionen\n",
    "def steep_valley_function():\n",
    "    # Funktionsterm\n",
    "    def f(x, y):\n",
    "        return 0.5 * x**2 + 2 * y**2\n",
    "\n",
    "    # Gradient der Funktion\n",
    "    def grad(x, y):\n",
    "        grad_x = x  # partielle Ableitung nach x\n",
    "        grad_y = 4 * y  # partielle Ableitung nach y\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    # Bauen der Funktion\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def l_shaped_valley_function():\n",
    "    # Funktionsterm\n",
    "    def f(x, y):\n",
    "        return np.where(x > y, (y + 20) ** 2 + x, (x + 20) ** 2 + y)\n",
    "\n",
    "    # Gradient der Funktion\n",
    "    def grad(x, y):\n",
    "        grad_x = np.where(x > y, 1, 2 * (x + 20)) # partielle Ableitung nach x\n",
    "        grad_y = np.where(x > y, 2 * (y + 20), 1) # partielle Ableitung nach y\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    # Bauen der Funktion\n",
    "    x = np.linspace(-50, 20, 400)\n",
    "    y = np.linspace(-50, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def asymmetric_convex_function():\n",
    "    # Funktionsterm\n",
    "    def f(x, y):\n",
    "        return (\n",
    "            0.3 * (x - 10) ** 2\n",
    "            + 0.2 * (y - 10) ** 2\n",
    "            + 1.1 * (x + 10) ** 2\n",
    "            + 1 * (y + 10) ** 2\n",
    "        )\n",
    "\n",
    "    # Gradient der Funktion\n",
    "    def grad(x, y):\n",
    "        grad_x = 0.6 * (x - 10) + 2.2 * (x + 10) # partielle Ableitung nach x \n",
    "        grad_y = 0.4 * (y - 10) + 2 * (y + 10) # partielle Ableitung nach y\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    # Bauen der Funktion\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def symmetric_convex_function():\n",
    "    # Funktionsterm\n",
    "    def f(x, y):\n",
    "        return x**2 + y**2 \n",
    "\n",
    "    # Gradient der Funktion\n",
    "    def grad(x, y):\n",
    "        grad_x = 2 * x # partielle Ableitung nach x \n",
    "        grad_y = 2 * y # partielle Ableitung nach y\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    # Bauen der Funktion\n",
    "    x = np.linspace(-20, 20, 100)\n",
    "    y = np.linspace(-20, 20, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "# Learning Rate Scheduling\n",
    "def power_scheduling(initial_lr, t, s, c=1):\n",
    "    return initial_lr / (1 + t / s) ** c\n",
    "\n",
    "def exponential_scheduling(initial_lr, t, s):\n",
    "    return initial_lr * (0.1 ** (t / s))\n",
    "\n",
    "def piecewise_constant_scheduling(epoch, boundaries, values):\n",
    "    for boundary, value in zip(boundaries, values):\n",
    "        if epoch < boundary:\n",
    "            return value\n",
    "    return values[-1]\n",
    "\n",
    "def performance_scheduling(validation_error, prev_validation_error, lr, lambda_factor=0.1):\n",
    "    if validation_error > prev_validation_error:\n",
    "        return lr * lambda_factor\n",
    "    return lr\n",
    "\n",
    "def one_cycle_scheduling(t, total_steps, initial_lr, max_lr):\n",
    "    if t < total_steps / 2:\n",
    "        return initial_lr + (max_lr - initial_lr) * (2 * t / total_steps)\n",
    "    return max_lr - (max_lr - initial_lr) * (2 * (t - total_steps / 2) / total_steps)\n",
    "\n",
    "# Anwenden des Schedulers auf Learning Rate in Schritt t\n",
    "def apply_lr_scheduler(lr_scheduler, t, lr, scheduler_params):\n",
    "    if lr_scheduler is None or lr_scheduler == \"None\":\n",
    "        return lr\n",
    "    elif lr_scheduler == \"Power\":\n",
    "        return power_scheduling(lr, t, scheduler_params[\"Power\"])\n",
    "    elif lr_scheduler == \"Exponential\":\n",
    "        return exponential_scheduling(lr, t, 2)  # 2 als default fuer s\n",
    "    elif lr_scheduler == \"Piecewise\":\n",
    "        boundaries = list(map(int, scheduler_params[\"Piecewise Boundaries\"].split(',')))\n",
    "        values = list(map(float, scheduler_params[\"Piecewise Values\"].split(',')))\n",
    "        return piecewise_constant_scheduling(t, boundaries, values)\n",
    "    elif lr_scheduler == \"Performance\":\n",
    "        prev_validation_error = 0  # Platzhalter, da Loss nicht beruecksichtigt wird\n",
    "        return performance_scheduling(0, prev_validation_error, lr, scheduler_params[\"Lambda Factor\"])\n",
    "    elif lr_scheduler == \"OneCycle\":\n",
    "        total_steps = scheduler_params[\"Total Steps\"]\n",
    "        max_lr = scheduler_params[\"Max LR\"]\n",
    "        return one_cycle_scheduling(t, total_steps, lr, max_lr)\n",
    "    else:\n",
    "        return lr\n",
    "    \n",
    "# Standardisierte optimize-Funktion\n",
    "def optimize(\n",
    "    optimizer_type=\"Regular\",\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None,\n",
    "    print_res=False,\n",
    ") -> Tuple[Callable[[float, float], float], List[np.ndarray], List[float], str]:\n",
    "\n",
    "    # Fuehre den gewaehlten Optimizer aus\n",
    "    if optimizer_type == \"Regular\":\n",
    "        return regular_gradient_descent(function, steps, lr, cur_pos, print_res, lr_scheduler, scheduler_params)\n",
    "    elif optimizer_type == \"Momentum\":\n",
    "        return momentum_gradient_descent(function, steps, lr, beta1, cur_pos, print_res, lr_scheduler, scheduler_params)\n",
    "    elif optimizer_type == \"Nesterov\":\n",
    "        return nesterov_gradient_descent(function, steps, lr, beta1, cur_pos, print_res, lr_scheduler, scheduler_params)\n",
    "    elif optimizer_type == \"AdaGrad\":\n",
    "        return adagrad_gradient_descent(function, steps, lr, cur_pos, print_res, lr_scheduler, scheduler_params)\n",
    "    elif optimizer_type == \"RMSProp\":\n",
    "        return rmsprop_gradient_descent(function, steps, lr, beta1, cur_pos, print_res, lr_scheduler, scheduler_params)\n",
    "    elif optimizer_type == \"Adam\":\n",
    "        return adam_gradient_descent(\n",
    "            function, steps, lr, beta1, beta2, cur_pos, print_res, lr_scheduler, scheduler_params\n",
    "        )\n",
    "    elif optimizer_type == \"Custom\":\n",
    "        return custom_gradient_descent(\n",
    "            function, steps, lr, beta1, cur_pos, print_res, lr_scheduler, scheduler_params\n",
    "        )\n",
    "\n",
    "# Ausfuehren mehrerer Optimizer\n",
    "def optimize_multiple(\n",
    "    optimizers=[\"Regular\", \"momentum\"],\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    results = []\n",
    "    for optimizer in optimizers:\n",
    "        results.append(\n",
    "            optimize(optimizer, function, steps, lr, beta1, beta2, cur_pos, print_res)\n",
    "        )\n",
    "    return results\n",
    "\n",
    "# Plotting Funktion, die eine Liste von Ergebnissen nimmt und ein Plot zurueckgibt (noch nicht interaktiv)\n",
    "def plot_optimization_path(optimizer_results):\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection='3d') # 3D-Visualisierung\n",
    "    ax2 = fig.add_subplot(1, 2, 2)  # 2D Heatmap\n",
    "\n",
    "    # Funktion aus ersten Optimizer herleiten\n",
    "    f = optimizer_results[0][0] \n",
    "    x_range = np.linspace(-20, 20, 1000)\n",
    "    y_range = np.linspace(-20, 20, 1000)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = f(X, Y)\n",
    "\n",
    "    # Ermitteln des Minimums\n",
    "    min_z = np.min(Z)\n",
    "    min_pos = np.where(Z == min_z)\n",
    "    min_x = X[min_pos][0]\n",
    "    min_y = Y[min_pos][0]\n",
    "\n",
    "    # Plot 3D Visualisierung\n",
    "    ax1.plot_surface(X, Y, Z, alpha=0.5, cmap='viridis', edgecolor='none')\n",
    "    ax1.set_title('3D View - Optimization Paths')\n",
    "\n",
    "    # Markiere Minimum in 3D\n",
    "    ax1.scatter(min_x, min_y, min_z, marker='8', color='black', s=30, alpha=0.5, label=\"Minumum\")\n",
    "    #ax1.text(min_x, min_y, min_z, f'({min_x:.2f}, {min_y:.2f}, {min_z:.2f})', color='red')\n",
    "\n",
    "    # Plot 2D heatmap\n",
    "    ax2.contourf(X, Y, Z, levels=100, cmap='viridis')\n",
    "    ax2.set_title('Top-Down View (2D) - Optimization Paths')\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "\n",
    "    ax2.scatter(min_x, min_y, marker='8', color='black', linewidths=5, alpha=0.5, label=\"Minumum\")\n",
    "\n",
    "\n",
    "    # Farbschema fuer Optimizer\n",
    "    color_map = {\n",
    "        \"Regular\": \"red\",\n",
    "        \"Momentum\": \"blue\",\n",
    "        \"Nesterov\": \"green\",\n",
    "        \"AdaGrad\": \"orange\",\n",
    "        \"RMSProp\": \"purple\",\n",
    "        \"Adam\": \"black\"\n",
    "    }\n",
    "\n",
    "    # Plotten des Pfades anhand der positions-Liste\n",
    "    for idx, (f, positions, z_values, name) in enumerate(optimizer_results):\n",
    "        positions_array = np.array(positions)\n",
    "        color = color_map.get(name, \"gray\")  # Default to gray if name not found\n",
    "        \n",
    "        # 3D Pfad\n",
    "        ax1.scatter(\n",
    "            positions_array[:, 0], positions_array[:, 1], z_values,\n",
    "            color=color, s=50, label=name\n",
    "        )\n",
    "        \n",
    "        # 2D Pfad\n",
    "        ax2.plot(\n",
    "            positions_array[:, 0],\n",
    "            positions_array[:, 1],\n",
    "            marker='o',\n",
    "            color=color,\n",
    "            markersize=5,\n",
    "            linestyle='--',\n",
    "            linewidth=2,\n",
    "            label=name\n",
    "        )\n",
    "\n",
    "    # Bereich eingrenzen, um innerhalb von Definitionsbereich zu bleiben\n",
    "    ax1.set_xlim(-20,20)\n",
    "    ax1.set_ylim(-20,20)\n",
    "\n",
    "    ax2.set_xlim(-20,20)\n",
    "    ax2.set_ylim(-20,20)\n",
    "\n",
    "    # Legende\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion, welche Plots um interaktive Elemente ergaenzt\n",
    "def visualize_optimizer(optimizer_multiselect_options=[\"Regular\", \"Momentum\", \"Nesterov\", \"AdaGrad\", \"RMSProp\", \"Adam\"]):\n",
    "    # Optimizer Konfiguration\n",
    "    optimizer_settings = {\n",
    "        \"Regular\": {\"lr\": widgets.FloatSlider(value=0.1, min=0, max=2, step=0.01, description=\"Learning Rate\")},\n",
    "        \"Momentum\": {\n",
    "            \"lr\": widgets.FloatSlider(value=0.1, min=0, max=2, step=0.01, description=\"Learning Rate\"),\n",
    "            \"beta1\": widgets.FloatSlider(value=0.9, min=0, max=1, step=0.01, description=\"Beta1 Factor\")\n",
    "        },\n",
    "        \"Nesterov\": {\n",
    "            \"lr\": widgets.FloatSlider(value=0.1, min=0, max=2, step=0.01, description=\"Learning Rate\"),\n",
    "            \"beta1\": widgets.FloatSlider(value=0.9, min=0, max=1, step=0.01, description=\"Beta1 Factor\")\n",
    "        },\n",
    "        \"AdaGrad\": {\"lr\": widgets.FloatSlider(value=\"2\", min=0, max=2, step=0.01, description=\"Learning Rate\")},\n",
    "        \"RMSProp\": {\n",
    "            \"lr\": widgets.FloatSlider(value=\"2\", min=0, max=2, step=0.01, description=\"Learning Rate\"),\n",
    "            \"beta1\": widgets.FloatSlider(value=0.9, min=0, max=1, step=0.01, description=\"Beta1 Factor\")\n",
    "        },\n",
    "        \"Adam\": {\n",
    "            \"lr\": widgets.FloatSlider(value=\"2\", min=0, max=2, step=0.01, description=\"Learning Rate\"),\n",
    "            \"beta1\": widgets.FloatSlider(value=0.9, min=0, max=1, step=0.01, description=\"Beta1 Factor\"),\n",
    "            \"beta2\": widgets.FloatSlider(value=0.9, min=0, max=1, step=0.001, description=\"Beta2 Factor\")\n",
    "        },\n",
    "        \"Custom\": {\n",
    "            \"lr\": widgets.FloatSlider(value=\"2\", min=0, max=2, step=0.01, description=\"Learning Rate\"),\n",
    "            \"beta1\": widgets.FloatSlider(value=0.9, min=0, max=1, step=0.01, description=\"Beta1 Factor\")\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Nur ausgewaehlte Optimizer zur Konfiguration freigeben\n",
    "    optimizer_settings = {key: value for key, value in optimizer_settings.items() if key in optimizer_multiselect_options}\n",
    "\n",
    "    # Funktions-Auswahl\n",
    "    func_dropdown = widgets.Dropdown(\n",
    "        description=\"Function\",\n",
    "        options=[\n",
    "            (\"Steep Valley Function\", steep_valley_function),\n",
    "            (\"Symmetric Convex Function\", symmetric_convex_function),\n",
    "            (\"Asymmetric Convex Function\", asymmetric_convex_function),\n",
    "            (\"L-shaped Valley Function\", l_shaped_valley_function),\n",
    "        ],\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    x_slider = widgets.IntSlider(value=15, min=-20, max=20, step=1, description=\"Starting X\", layout=widgets.Layout(width=\"100%\"))\n",
    "    y_slider = widgets.IntSlider(value=10, min=-20, max=20, step=1, description=\"Starting Y\", layout=widgets.Layout(width=\"100%\"))\n",
    "    steps_slider = widgets.IntSlider(value=10, min=1, max=30, step=1, description=\"Steps\", layout=widgets.Layout(width=\"100%\"))\n",
    "\n",
    "    # Optimizer-Auswahl\n",
    "    optimizer_multiselect = widgets.SelectMultiple(\n",
    "        description=\"Optimizers\",\n",
    "        options=list(optimizer_settings.keys()),\n",
    "        value=[list(optimizer_settings.keys())[0]],\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "\n",
    "    # Toggle fuer Konfiguration einzelnder Optimizer\n",
    "    optimizer_toggle = widgets.ToggleButtons(\n",
    "        description=\"Configure Optimizer\",\n",
    "        options=list(optimizer_settings.keys()),\n",
    "        value=list(optimizer_settings.keys())[0],\n",
    "        layout=widgets.Layout(width=\"100%\")\n",
    "    )\n",
    "\n",
    "    # UI-Platzhalter fuer Konfiguration\n",
    "    optimizer_settings_box = widgets.VBox()\n",
    "\n",
    "    # Optimizer Konfiguration entsprechend Auswahl in UI einbinden\n",
    "    def update_optimizer_settings(change):\n",
    "        selected_optimizer = change['new']\n",
    "        boxes = [widgets.Label(value=selected_optimizer)]\n",
    "        for setting, widget in optimizer_settings[selected_optimizer].items():\n",
    "            boxes.append(widget)\n",
    "        optimizer_settings_box.children = boxes\n",
    "\n",
    "    optimizer_toggle.observe(update_optimizer_settings, names='value')\n",
    "\n",
    "    # Standard-Auswahl bei Initialisierung\n",
    "    update_optimizer_settings({'new': optimizer_toggle.value})\n",
    "\n",
    "    # Learning Rate Scheduler Auswahl\n",
    "    lr_schedulers = [\"None\", \"Power\", \"Exponential\", \"Piecewise\", \"Performance\", \"OneCycle\"]\n",
    "    lr_scheduler_dropdown = widgets.Dropdown(\n",
    "        description=\"LR Scheduler\",\n",
    "        options=lr_schedulers,\n",
    "        value=\"None\",\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "\n",
    "    # Aktualisieren der Plots\n",
    "    def update_plot(optimizers, function, steps, initialX, initialY,lr_scheduler, **kwargs):\n",
    "        cur_pos = np.array([initialX, initialY])\n",
    "        results = []\n",
    "        for optimizer in optimizers:\n",
    "            params = {key: kwargs.get(f'{optimizer}_{key}') for key in optimizer_settings[optimizer].keys()}\n",
    "            scheduler_params = {\"Power\": 2, \"Piecewise Boundaries\": \"10,20\", \"Piecewise Values\": \"0.1,0.01,0.001\", \"Lambda Factor\": 0.1, \"Total Steps\": 12, \"Max LR\": 0.2}\n",
    "            results.append(optimize(optimizer, function, steps, cur_pos=cur_pos, lr_scheduler=lr_scheduler, scheduler_params=scheduler_params, **params))\n",
    "        plot_optimization_path(results)\n",
    "\n",
    "    # Verknuepfen von UI zu Plots\n",
    "    out = widgets.interactive_output(\n",
    "        update_plot,\n",
    "        {\n",
    "            \"optimizers\": optimizer_multiselect,\n",
    "            \"function\": func_dropdown,\n",
    "            \"steps\": steps_slider,\n",
    "            \"initialX\": x_slider,\n",
    "            \"initialY\": y_slider,\n",
    "            \"lr_scheduler\": lr_scheduler_dropdown,\n",
    "            **{f'{opt}_{param}': widget for opt, settings in optimizer_settings.items() for param, widget in settings.items()}\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # UI-Elemente\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([x_slider, y_slider]),\n",
    "        widgets.HBox([steps_slider, func_dropdown]),\n",
    "        optimizer_multiselect,\n",
    "        optimizer_toggle,\n",
    "        optimizer_settings_box,\n",
    "        lr_scheduler_dropdown\n",
    "    ])\n",
    "\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschnitt 1: Regular vs Momentum Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Hier werden die Functions für den Step und Gradient Descent der zwei ersten Optimizer gesetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Regular Gradient Descent Step\n",
    "\n",
    "\n",
    "def regular_gradient_step(pos, lr, grad):\n",
    "    return pos - lr * grad(*pos)\n",
    "\n",
    "\n",
    "def regular_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None,\n",
    ") -> Tuple[Callable[[float, float], float], List[np.ndarray], List[float], str]:\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        cur_pos = regular_gradient_step(cur_pos, lr, grad)\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # Um Punkte außerhalb von Plot zu vermeiden\n",
    "        positions.append(cur_pos)\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Regular\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Momentum Gradient Descent Step\n",
    "\n",
    "\n",
    "def momentum_step(pos, lr, beta, momentum, grad):\n",
    "    gradient = grad(*pos)\n",
    "    momentum = beta * momentum - lr * gradient\n",
    "    pos = pos + momentum\n",
    "\n",
    "    return momentum, pos\n",
    "\n",
    "\n",
    "def momentum_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    momentum = np.zeros(2)\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        momentum, cur_pos = momentum_step(cur_pos, lr, beta1, momentum, grad)\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # Um Punkte außerhalb von Plot zu vermeiden\n",
    "        positions.append(cur_pos)\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Momentum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe / Visualisieren\n",
    "Die Aufgabe in Abschnitt 1 besteht darin, mit den Optimierern Regular und Momentum herumzuspielen und sich mit dem Visualisierungstool vertraut zu machen.\n",
    "\n",
    "Versuchen Sie, diese Fragen (für sich selbst) zu beantworten:\n",
    "- Wie verhalten sich die Optimierer bei unterschiedlichen ``Starting X``- und ``Starting Y``-Werten?\n",
    "- Wie verändert sich das Verhalten des Optimierers, wenn die ``Learning Rate`` angepasst wird?\n",
    "- Welcher Teil des Verhaltens des Optimierers wird durch den ``Beta1-Faktor`` beeinflusst?\n",
    "- Wie verhält sich der Optimierer bei verschiedenen ``Function(s)``?\n",
    "\n",
    "Tipps:\n",
    "- Sie können mehrere Optimierer auf einmal auswählen, indem Sie die Umschalttaste während der Auswahl gedrückt halten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5491724d23ab46a8aa94936af917dc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413c52b10d9346c2a0584a76552bb9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_optimizer(optimizer_multiselect_options=[\"Regular\", \"Momentum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschnitt 2: Nesterov Accelerated Gradient (NAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Setup)\n",
    "Diese Section bedarf keines weiteren Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe\n",
    "Ihre Aufgabe ist es, die Schrittfunktion für den Nesterov Accelerated Gradient Descent (kurz: NAG) zu definieren.\n",
    "\n",
    "Tipps:\n",
    "- Schauen Sie sich die mathematische Definition genau an\n",
    "- Orientieren Sie sich an der Schrittfunktion des Momentum-Optimierers\n",
    "\n",
    "Nesterov Equation:\n",
    "$$ m \\leftarrow \\beta m - \\eta \\nabla_{\\theta} J(\\theta + \\beta m) $$\n",
    "$$ \\theta \\leftarrow \\theta + m $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov_step(pos, lr, beta, momentum, grad):\n",
    "    # TODO Implementiere die Nesterov Step Funktion\n",
    "    # Wichtig: Der Parameter grad ist eine Funktion, die beim Aufruf von grad(*x) den Wert des Gradienten an der Position x angibt.\n",
    "    return pos, momentum\n",
    "\n",
    "\n",
    "def nesterov_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    momentum = np.zeros(2)  # Momentum bei Initialisiern\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        cur_pos, momentum = nesterov_step(cur_pos, lr, beta1, momentum, grad)\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # Um Punkte außerhalb von Plot zu vermeiden\n",
    "        positions.append(cur_pos.copy())  # Speichere neue Position\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Nesterov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd794abb96b4ce781003aef9adb51cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a88ccb4fb74ba89ba9da66d444e3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_optimizer(optimizer_multiselect_options=['Nesterov'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschnitt 3: AdaGrad, RMSProp und Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe\n",
    "Ihre Aufgabe ist es, die Variable s sowohl für Adagrad als auch für RMSProp zu visualisieren. \n",
    "Denken Sie an die folgenden Fragen:\n",
    "- Welcher Unterschied ist erkennbar, warum ist das der Fall?\n",
    "- Welche Rolle spielt s in beiden Optimierern?\n",
    "\n",
    "### Formeln:\n",
    "AdaGrad\n",
    "$$ s \\leftarrow s + \\nabla_{\\theta} J(\\theta)^2 $$\n",
    "$$ \\theta \\leftarrow \\theta - \\frac{\\eta}{\\sqrt{s + \\epsilon}} \\cdot \\nabla_{\\theta} J(\\theta)  $$\n",
    "\n",
    "RMSProp \n",
    "$$ \\mathbf{s} \\leftarrow \\beta \\mathbf{s} + (1-\\beta) \\nabla_{\\theta} J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) $$\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta) \\oslash \\sqrt{\\mathbf{s} + \\epsilon} $$\n",
    "\n",
    "Tipps:\n",
    "- Verwende die globalen Listen adagrad_s und rmsprop_s\n",
    "- Die Plot-Funktion ist bereits unter dem Block implementiert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad_s = []\n",
    "rmsprop_s = []\n",
    "\n",
    "def adagrad_step(pos, lr, grad, s):\n",
    "    epsilon = 10 ** (-10)  # Abaendern, falls benoetigt\n",
    "    gradient = grad(*pos)\n",
    "    s += gradient**2  # akkumulierten quadrierten Gradienten\n",
    "    adjusted_lr = lr / (np.sqrt(s) + epsilon)\n",
    "    pos = pos - adjusted_lr * gradient\n",
    "    return pos, s\n",
    "\n",
    "\n",
    "def adagrad_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "    global adagrad_s\n",
    "    s = np.zeros(2)  # Initiale Summe der Gradienten\n",
    "    adagrad_s = []\n",
    "    for i in range(steps):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        cur_pos, s = adagrad_step(cur_pos, lr, grad, s)\n",
    "        #adagrad_s.append(s.copy())\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # Um Punkte außerhalb von Plot zu vermeiden\n",
    "        positions.append(cur_pos.copy())  # Speichere aktuelle Position\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"AdaGrad\")\n",
    "\n",
    "\n",
    "def rmsprop_step(pos, lr, grad, s, beta=0.9, epsilon=1e-8):\n",
    "    gradient = grad(*pos)\n",
    "    s = beta * s + (1 - beta) * gradient**2\n",
    "    pos = pos - (lr * gradient / (np.sqrt(s + epsilon)))\n",
    "    return pos, s\n",
    "\n",
    "    # # alternative function with adjusted learning rate\n",
    "    # gradient = grad(*pos)\n",
    "    # s = beta * s + (1 - beta) * gradient ** 2  # Update accumulation of squared gradients\n",
    "    # adjusted_lr = lr / (np.sqrt(s) + epsilon)  # Adjust learning rate\n",
    "    # pos = pos - adjusted_lr * gradient  # Update position\n",
    "    # return pos, s\n",
    "\n",
    "\n",
    "def rmsprop_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "    global rmsprop_s\n",
    "    s = np.zeros_like(cur_pos)  \n",
    "    rmsprop_s = []\n",
    "    for i in range(steps):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        cur_pos, s = rmsprop_step(\n",
    "            cur_pos, lr, grad, s, beta1\n",
    "        )  # RMS Step\n",
    "        #rmsprop_s.append(s.copy())\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # Um Punkte außerhalb von Plot zu vermeiden\n",
    "        positions.append(cur_pos.copy()) # Speichere aktuelle Position\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"RMSProp\")\n",
    "\n",
    "\n",
    "def adam_step(pos, lr, grad, m, v, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    gradient = grad(*pos)\n",
    "    m = beta1 * m + (1 - beta1) * gradient  # Aktualisierung der verzerrten Schaetzung des ersten Moments\n",
    "    v = beta2 * v + (1 - beta2) * gradient**2  # Aktualisierung der verzerrten Schaetzung des zweiten Moments\n",
    "    m_hat = m / (1 - beta1**t)  # Berechnung der verzerrungskorrigierten Schaetzung des ersten Moments\n",
    "    v_hat = v / (1 - beta2**t)  # Berechnung der verzerrungskorrigierten Schaetzung des zweiten Moments\n",
    "    pos = pos - lr * m_hat / (np.sqrt(v_hat) + epsilon)  # Aktualisiere Parameter\n",
    "    return pos, m, v\n",
    "\n",
    "\n",
    "def adam_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.99,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    m = np.zeros_like(cur_pos)  # Initialisierung Vektor ersten Moments\n",
    "    v = np.zeros_like(cur_pos)  # Initialisierung Vektor zweiten Moments\n",
    "    t = 0  # Initialisierung timestep\n",
    "\n",
    "    positions = [cur_pos.copy()]  # Speichere Start-Position\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        t += 1  # Erhoehen timestep\n",
    "        cur_pos, m, v = adam_step(\n",
    "            cur_pos, lr, grad, m, v, t, beta1, beta2\n",
    "        )  # Adam Step\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # Um Punkte außerhalb von Plot zu vermeiden\n",
    "        positions.append(cur_pos.copy())  # Speichere aktuelle Position\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed58fcaf1a6c4852a0ea35aa905d0bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1786fe150844e38b2e50d130014d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_optimizer(optimizer_multiselect_options=[\"AdaGrad\", \"RMSProp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Run the optimizers first",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m Run the optimizers first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if not np.array(adagrad_s).any() or not np.array(rmsprop_s).any():\n",
    "    raise SystemExit('Verwende erste beide Optimizer in der Visualisierung')\n",
    "\n",
    "# Konvertierte Listen\n",
    "adagrad_s = np.array(adagrad_s)\n",
    "rmsprop_s = np.array(rmsprop_s)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot fuer jede Dimenstion\n",
    "plt.plot(adagrad_s[:, 0], label='AdaGrad - X', marker='o', color='orange')\n",
    "plt.plot(adagrad_s[:, 1], label='AdaGrad - Y', marker='o', linestyle='dashed', color='orange')\n",
    "plt.plot(rmsprop_s[:, 0], label='RMSProp - X', marker='x', color='purple')\n",
    "plt.plot(rmsprop_s[:, 1], label='RMSProp - Y', marker='x', linestyle='dashed', color='purple')\n",
    "\n",
    "plt.title('Evolution of s over Steps for AdaGrad and RMSProp')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('s')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section X: Eigener Optimizer\n",
    "In dieser Section sollen Sie ihren eigenen Optimizer implementieren mit dem Haken, dass Zufall eingebunden werden muss\n",
    "\n",
    "Ziel ist es als schnellste(r) Richtung Optimum zu gelangen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7defc8677a4cc08789ac819ad03e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eede616980fe490eb8987994389b483b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def custom_step(pos, lr, grad):\n",
    "    pos = pos - lr * grad(*pos)\n",
    "    # TODO Implementiere eigene Logik\n",
    "    # Zufall kann durch randint(a,b) eingebunden werden\n",
    "    return pos\n",
    "\n",
    "\n",
    "def custom_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    positions = [cur_pos.copy()]  # Speichern der Start-Positionen\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        cur_pos = custom_step(cur_pos, lr, grad)  # Custom Step\n",
    "        cur_pos = np.clip(cur_pos, -20, 20) # Um Punkte außerhalb des Plots zu vermeiden\n",
    "        positions.append(cur_pos.copy())  # Speichere aktuelle Position\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Custom\")\n",
    "\n",
    "visualize_optimizer([\"Custom\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_lars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
