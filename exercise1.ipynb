{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def random_initialize():\n",
    "    x = randint(-20, 20)\n",
    "    y = randint(-20,20)\n",
    "    return np.meshgrid(x,y)\n",
    "\n",
    "def steep_valley_function():\n",
    "    def f(x, y):\n",
    "        # Define a function with a steeper gradient in the y-direction\n",
    "        return 0.5 * x**2 + 2 * y**2\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Compute the gradient of the function\n",
    "        grad_x = x  # Gradient with respect to x\n",
    "        grad_y = 4 * y  # Gradient with respect to y, steeper than x\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "def l_shaped_valley_function():\n",
    "    def f(x, y):\n",
    "        # Creating an L-shaped valley with a sharp turn\n",
    "        # Define the valley with two different linear regions\n",
    "        # Horizontal part (x > y), Vertical part (x <= y)\n",
    "        return np.where(x > y, (y + 20)**2 + x, (x + 20)**2 + y)\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Gradient of the L-shaped function, changing at x = y\n",
    "        # Horizontal part gradient\n",
    "        grad_x = np.where(x > y, 1, 2 * (x + 20))\n",
    "        grad_y = np.where(x > y, 2 * (y + 20), 1)\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-50, 20, 400)\n",
    "    y = np.linspace(-50, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "def complex_function():\n",
    "    def f(x, y):\n",
    "        # Scale the coefficients to increase the depth and curvature differences\n",
    "        return 0.3 * (x - 10)**2 + 0.2 * (y - 10)**2 + 1 * (x + 10)**2 + 1 * (y + 10)**2\n",
    "\n",
    "    def grad(x, y):\n",
    "        # Adjusted gradient for the scaled function\n",
    "        grad_x = 0.6 * (x - 10) + 2 * (x + 10)  # Increased coefficients for the gradients\n",
    "        grad_y = 0.4 * (y - 10) + 2 * (y + 10)\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "def simple_function():\n",
    "    def f(x, y):\n",
    "        return x**2+y**2  # Z as a function of X and Y\n",
    "    def grad(x, y):\n",
    "        grad_x = 2*x\n",
    "        grad_y = 2*y\n",
    "        return np.array([grad_x, grad_y])\n",
    "    x = np.linspace(-20, 20, 100)\n",
    "    y = np.linspace(-20, 20, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "def build_output(method='gradient'):\n",
    "\n",
    "    lr_slider = widgets.FloatSlider(\n",
    "        value=0.1, min=0, max=2, step=0.01, \n",
    "        description=\"Learning Rate\",\n",
    "        style={'description_width': 'initial'},  # Ensure description does not get cut off\n",
    "        layout=widgets.Layout(width='100%')  # Full width of the container\n",
    "    )\n",
    "\n",
    "    steps_slider = widgets.IntSlider(\n",
    "        value=10, min=1, max=30, step=1, \n",
    "        description=\"Steps\",\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='100%')\n",
    "    )\n",
    "\n",
    "    func_dropdown = widgets.Dropdown(\n",
    "        description=\"Function\", \n",
    "        options=[('simple Function', simple_function), ('complex Function', complex_function), ('L-shaped Valley Function', l_shaped_valley_function), ('Steep Valley Function',steep_valley_function)],  # Use a list of tuples for clarity\n",
    "        layout=widgets.Layout(width='50%')  # Adjust as needed based on UI considerations\n",
    "    )\n",
    "\n",
    "    x_slider = widgets.IntSlider(\n",
    "        value=15, min=-20, max=20, step=1, \n",
    "        description=\"Starting X\",\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    y_slider = widgets.IntSlider(\n",
    "        value=10, min=-20, max=20, step=1, \n",
    "        description=\"Starting Y\",\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    beta_slider = widgets.FloatSlider(\n",
    "        value=0.9, min=-0, max=1, step=0.01, \n",
    "        description=\"Beta Factor\",\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    # Create a horizontal box to hold the widgets\n",
    "\n",
    "\n",
    "    match method:\n",
    "        case 'gradient':\n",
    "            ui = widgets.HBox([x_slider, y_slider, lr_slider, steps_slider, func_dropdown], \n",
    "                layout=widgets.Layout(flex_flow='row wrap'))  # Wrap the content if not enough space\n",
    "                \n",
    "            out = widgets.interactive_output(regular_gradient_descent, {\n",
    "            'initialX': x_slider, 'initialY': y_slider, 'lr': lr_slider, \n",
    "            'steps': steps_slider, 'function': func_dropdown,\n",
    "            'print_res': widgets.fixed(False), 'initialRandom': widgets.fixed(False)\n",
    "            })\n",
    "        case 'momentum':\n",
    "            ui = widgets.HBox([x_slider, y_slider, lr_slider, steps_slider,beta_slider, func_dropdown], \n",
    "                layout=widgets.Layout(flex_flow='row wrap'))  # Wrap the content if not enough space\n",
    "            out = widgets.interactive_output(momentum_gradient_descent, {\n",
    "            'initialX': x_slider, 'initialY': y_slider, 'lr': lr_slider, \n",
    "            'steps': steps_slider, 'function': func_dropdown,\n",
    "            'beta': beta_slider,\n",
    "            'print_res': widgets.fixed(False), 'initialRandom': widgets.fixed(False)\n",
    "            })\n",
    "        case 'nesterov':\n",
    "            ui = widgets.HBox([x_slider, y_slider, lr_slider, steps_slider,beta_slider, func_dropdown], \n",
    "                layout=widgets.Layout(flex_flow='row wrap'))  # Wrap the content if not enough space\n",
    "            out = widgets.interactive_output(nesterov_gradient_descent, {\n",
    "            'initialX': x_slider, 'initialY': y_slider, 'lr': lr_slider, \n",
    "            'steps': steps_slider, 'function': func_dropdown,\n",
    "            'beta': beta_slider,\n",
    "            'print_res': widgets.fixed(False), 'initialRandom': widgets.fixed(False)\n",
    "            })\n",
    "        case 'adagrad':\n",
    "            ui = widgets.HBox([x_slider, y_slider, lr_slider, steps_slider,func_dropdown], \n",
    "                layout=widgets.Layout(flex_flow='row wrap'))  # Wrap the content if not enough space\n",
    "            out = widgets.interactive_output(adagrad_gradient_descent, {\n",
    "            'initialX': x_slider, 'initialY': y_slider, 'lr': lr_slider, \n",
    "            'steps': steps_slider, 'function': func_dropdown,\n",
    "            'print_res': widgets.fixed(False), 'initialRandom': widgets.fixed(False)\n",
    "            })\n",
    "        case 'rmsprop':\n",
    "            ui = widgets.HBox([\n",
    "                x_slider, y_slider, lr_slider, steps_slider, beta_slider, func_dropdown\n",
    "            ], layout=widgets.Layout(flex_flow='row wrap'))  # Arrange widgets in a row, wrap if space is insufficient\n",
    "            \n",
    "            out = widgets.interactive_output(rmsprop_gradient_descent, {\n",
    "                'initialX': x_slider, 'initialY': y_slider, 'lr': lr_slider,\n",
    "                'steps': steps_slider, 'beta': beta_slider, 'function': func_dropdown,\n",
    "                'print_res': widgets.fixed(False), 'initialRandom': widgets.fixed(False)\n",
    "            })\n",
    "        case 'adam':\n",
    "            ui = widgets.HBox([x_slider, y_slider, lr_slider, steps_slider, func_dropdown], \n",
    "                            layout=widgets.Layout(flex_flow='row wrap'))\n",
    "            out = widgets.interactive_output(adam_gradient_descent, {\n",
    "                'initialX': x_slider, 'initialY': y_slider, 'lr': lr_slider,\n",
    "                'steps': steps_slider, 'function': func_dropdown,\n",
    "                'print_res': widgets.fixed(False), 'initialRandom': widgets.fixed(False)\n",
    "            })\n",
    "\n",
    "\n",
    "    \n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optimization_path(X, Y, Z, positions, function, function_name):\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax2 = fig.add_subplot(1, 2, 2)  # 2D plot for top-down heatmap view\n",
    "    \n",
    "    # Plot the 3D surface\n",
    "    ax1.plot_surface(X, Y, Z, alpha=0.5, cmap='viridis', edgecolor='none')\n",
    "    ax1.set_title(\"3D View - \" + function_name)\n",
    "\n",
    "    # Convert positions to an array for easier manipulation\n",
    "    positions_array = np.array(positions)\n",
    "    # Calculate Z values for each X, Y position\n",
    "    z_values = np.array([function(pos[0], pos[1]) for pos in positions_array])\n",
    "\n",
    "    # Plotting the path on the 3D plot\n",
    "    ax1.scatter(positions_array[:, 0], positions_array[:, 1], z_values, color=\"red\", s=50)\n",
    "\n",
    "    # Create a contour plot for the 2D view\n",
    "    ax2.contourf(X, Y, Z, levels=100, cmap='viridis')\n",
    "    ax2.plot(positions_array[:, 0], positions_array[:, 1], marker='o', color='red', markersize=5, linestyle='--', linewidth=2)\n",
    "    ax2.set_title(\"Top-Down View (2D) - \" + function_name)\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623a601fe44e4ee7a061276145fa86a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='50%'), max=20, min=-20), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648529411cc6413f85acea88bf4f9819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def regular_gradient_step(pos, lr, grad):\n",
    "    return pos - lr * grad(*pos)\n",
    "\n",
    "def regular_gradient_descent(function=steep_valley_function, steps=1, lr=0.1, print_res=False, initialRandom=False, initialX=0, initialY=0):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    cur_pos = np.array([initialX, initialY])\n",
    "    positions = [cur_pos]\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos = regular_gradient_step(cur_pos, lr, grad)\n",
    "        positions.append(cur_pos)\n",
    "        if print_res:\n",
    "            cur_z = f(cur_pos[0], cur_pos[1])\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    plot_optimization_path(X, Y, Z, positions, f, \"Regular Gradient Descent\")\n",
    "    \n",
    "build_output(method='gradient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "m \\leftarrow \\beta m - \\eta \\nabla_{\\theta} J(\\theta) \\\\\n",
    "\\theta \\leftarrow \\theta + m\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d383f4dbcb0e4e34a184ee0e2bec135b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='50%'), max=20, min=-20), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59015b9967d14f7ba37f10e068753eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def momentum_step(pos, lr, beta, momentum, grad):\n",
    "    gradient = grad(*pos)\n",
    "    momentum = beta * momentum - lr * gradient\n",
    "    pos = pos + momentum\n",
    "\n",
    "    return momentum, pos\n",
    "\n",
    "def momentum_gradient_descent(function=simple_function, steps=10, lr=0.1, beta=0.1, print_res=False, initialRandom=True, initialX=10, initialY=10):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    cur_pos = random_initialize() if initialRandom else np.array([initialX, initialY])\n",
    "    momentum = np.zeros(2)\n",
    "    positions = [cur_pos]\n",
    "\n",
    "    for i in range(steps):\n",
    "        momentum, cur_pos = momentum_step(cur_pos, lr, beta, momentum, grad)\n",
    "        positions.append(cur_pos)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}\")\n",
    "\n",
    "    plot_optimization_path(X, Y, Z, positions, f, \"Momentum Gradient Descent\")\n",
    "\n",
    "build_output(method='momentum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't yet work with new plot function\n",
    "# print(\"Regular Gradient-Descent\")\n",
    "# regular_gradient_descent(steps=10, initialRandom=False, initialX=25, print_res=False)\n",
    "# print(\"Momentum Optimization\")\n",
    "# momentum_gradient_descent(steps=10,beta=0.5, initialRandom=False, initialX=25, print_res=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ m \\leftarrow \\beta m - \\eta \\nabla_{\\theta} J(\\theta + \\beta m) $$\n",
    "$$ \\theta \\leftarrow \\theta + m $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1844da550e4a368e2c094af9150691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='50%'), max=20, min=-20), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8485d58db67b4e8fa3b986643f2ab3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def nesterov_step(pos, lr, beta, momentum, grad):\n",
    "    lookahead_pos = pos + beta * momentum #Use the gradient slightly ahead in the direction of the momentum\n",
    "    gradient = grad(*lookahead_pos)\n",
    "    momentum = beta * momentum - lr * gradient\n",
    "    pos = pos + momentum\n",
    "    \n",
    "    return momentum, pos\n",
    "\n",
    "def nesterov_gradient_descent(function=steep_valley_function, steps=10, lr=0.1, beta=0.1, print_res=False, initialRandom=False, initialX=10, initialY=10):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    if initialRandom:\n",
    "        cur_pos = random_initialize()\n",
    "    else:\n",
    "        cur_pos = np.array([initialX, initialY])  # Starting position as a 2D vector\n",
    "\n",
    "    momentum = np.zeros(2)  # Initial momentum vector\n",
    "    positions = [cur_pos.copy()]  # List to store positions for plotting\n",
    "\n",
    "    for i in range(steps):\n",
    "        momentum, cur_pos = nesterov_step(cur_pos, lr, beta, momentum, grad)\n",
    "        positions.append(cur_pos.copy())  # Append the new position after update\n",
    "        if print_res:\n",
    "            cur_z = f(cur_pos[0], cur_pos[1])\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    # Use the generic plotting function to visualize the path\n",
    "    plot_optimization_path(X, Y, Z, positions, f, \"Nesterov Gradient Descent\")\n",
    "\n",
    "build_output(method='nesterov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{s} \\leftarrow \\mathbf{s} + \\nabla_{\\theta} J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) $$\n",
    "$$ \\theta \\leftarrow \\theta - \\eta  \\nabla_{\\theta} J(\\theta) \\oslash \\sqrt{\\mathbf{s} + \\epsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d1a7ff753746beaa7f64b7909f42bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='50%'), max=20, min=-20), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf35279e51d4b44bf822c2f41bfa1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def adagrad_step(pos, lr, grad, s):\n",
    "    epsilon = 10**(-10) #set to different value if needed\n",
    "    gradient = grad(*pos)\n",
    "    s = s + gradient ** 2  # Accumulate squared gradients\n",
    "    adjusted_lr = lr / (np.sqrt(s) + epsilon)\n",
    "    pos = pos - adjusted_lr * gradient\n",
    "    \n",
    "    return pos, s\n",
    "\n",
    "def adagrad_gradient_descent(function=simple_function, steps=10, lr=0.1, print_res=False, initialRandom=True, initialX=10, initialY=10):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    if initialRandom:\n",
    "        cur_pos = random_initialize()\n",
    "    else:\n",
    "        cur_pos = np.array([initialX, initialY])  # Starting position as a 2D vector\n",
    "\n",
    "    s = np.zeros(2)  # Initialize sum of squares of gradients\n",
    "    positions = [cur_pos.copy()]  # Store positions for plotting\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos, s = adagrad_step(cur_pos, lr, grad, s)\n",
    "        positions.append(cur_pos.copy())  # Append the new position after update\n",
    "        if print_res:\n",
    "            cur_z = f(cur_pos[0], cur_pos[1])\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    # Use the generic plotting function to visualize the path\n",
    "    plot_optimization_path(X, Y, Z, positions, f, \"AdaGrad Gradient Descent\")\n",
    "\n",
    "\n",
    "build_output(method='adagrad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{s} \\leftarrow \\beta \\mathbf{s} + (1-\\beta) \\nabla_{\\theta} J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) $$\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta) \\oslash \\sqrt{\\mathbf{s} + \\epsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e319a85924594c3a91566a7700758ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='50%'), max=20, min=-20), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef972a19d30f4282bf3c67a01a5a18f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rmsprop_step(pos, lr, grad, s, beta=0.9, epsilon=1e-8):\n",
    "    # Compute the gradient at the current position\n",
    "    gradient = grad(*pos)\n",
    "\n",
    "    # Update the moving average of the squared gradients\n",
    "    s = beta * s + (1 - beta) * gradient * gradient # gradient**2 also possible\n",
    "\n",
    "    pos = pos - (lr * gradient / (np.sqrt(s + epsilon)))\n",
    "\n",
    "    return pos, s\n",
    "\n",
    "    # # alternative function with adjusted learning rate\n",
    "    # gradient = grad(*pos)\n",
    "    # s = beta * s + (1 - beta) * gradient ** 2  # Update accumulation of squared gradients\n",
    "    # adjusted_lr = lr / (np.sqrt(s) + epsilon)  # Adjust learning rate\n",
    "    # pos = pos - adjusted_lr * gradient  # Update position\n",
    "    # return pos, s\n",
    "\n",
    "def rmsprop_gradient_descent(function=simple_function, steps=10, lr=0.1, beta=0.9, print_res=False, initialRandom=True, initialX=10, initialY=10):\n",
    "    X, Y, Z, f, grad = function()  # Load the function's landscape and gradient\n",
    "    if initialRandom:\n",
    "        # If random initialization is specified\n",
    "        cur_pos = np.array([random.randint(-20, 20), random.randint(-20, 20)])\n",
    "    else:\n",
    "        cur_pos = np.array([initialX, initialY])  # Use the specified initial position\n",
    "\n",
    "    s = np.zeros_like(cur_pos)  # Initialize the RMS accumulation variable\n",
    "    positions = [cur_pos.copy()]  # List to collect all positions for visualization\n",
    "\n",
    "    for i in range(steps):\n",
    "        cur_pos, s = rmsprop_step(cur_pos, lr, grad, s, beta)  # Update position and RMS\n",
    "        positions.append(cur_pos.copy())  # Append the new position for plotting\n",
    "        if print_res:\n",
    "            cur_z = f(*cur_pos)\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    # Call the plot function at the end of all iterations\n",
    "    plot_optimization_path(X, Y, Z, positions, f, \"RMSProp Gradient Descent\")\n",
    "\n",
    "\n",
    "\n",
    "build_output(method='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{m} \\leftarrow \\beta_1 \\mathbf{m} + (1 - \\beta_1) \\nabla_{\\theta} J(\\theta) $$\n",
    "$$ \\mathbf{s} \\leftarrow \\beta_2 \\mathbf{s} + (1 - \\beta_2) \\nabla_{\\theta} J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) $$\n",
    "$$ \\widehat{\\mathbf{m}} \\leftarrow \\frac{\\mathbf{m}}{1 - \\beta_1^t}, \\quad \\widehat{\\mathbf{s}} \\leftarrow \\frac{\\mathbf{s}}{1 - \\beta_2^t} $$\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\widehat{\\mathbf{m}} \\oslash \\sqrt{\\widehat{\\mathbf{s}} + \\epsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5b01afe0c949cdb14ffa6bfa51ec66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='50%'), max=20, min=-20), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb668b1f42d74bd0af5a894ae646b1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def adam_step(pos, lr, grad, m, v, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    gradient = grad(*pos)\n",
    "    m = beta1 * m + (1 - beta1) * gradient  # Update biased first moment estimate\n",
    "    v = beta2 * v + (1 - beta2) * gradient**2  # Update biased second moment estimate\n",
    "    m_hat = m / (1 - beta1**t)  # Compute bias-corrected first moment estimate\n",
    "    v_hat = v / (1 - beta2**t)  # Compute bias-corrected second moment estimate\n",
    "    pos = pos - lr * m_hat / (np.sqrt(v_hat) + epsilon)  # Update parameters\n",
    "    return pos, m, v\n",
    "\n",
    "def adam_gradient_descent(function=simple_function, steps=10, lr=0.1, print_res=False, initialRandom=True, initialX=10, initialY=10):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    if initialRandom:\n",
    "        cur_pos = np.array([np.random.randint(-20, 20), np.random.randint(-20, 20)])\n",
    "    else:\n",
    "        cur_pos = np.array([initialX, initialY])\n",
    "\n",
    "    m = np.zeros_like(cur_pos)  # Initialize first moment vector\n",
    "    v = np.zeros_like(cur_pos)  # Initialize second moment vector\n",
    "    t = 0  # Initialize timestep\n",
    "\n",
    "    positions = [cur_pos.copy()]  # Store positions for plotting\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        t += 1  # Increment time step\n",
    "        cur_pos, m, v = adam_step(cur_pos, lr, grad, m, v, t)  # Adam optimization step\n",
    "        positions.append(cur_pos.copy())\n",
    "        if print_res:\n",
    "            cur_z = f(*cur_pos)\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    # Use the plot function to visualize the optimization path\n",
    "    plot_optimization_path(X, Y, Z, positions, f, \"Adam Gradient Descent\")\n",
    "\n",
    "\n",
    "build_output(method='adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
